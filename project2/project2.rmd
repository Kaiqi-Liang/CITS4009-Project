---
title: "CITS4009 Computational Data Analysis"
subtitle: "Project 2 - Modelling"

graphics: yes
author: <i>Kaiqi LIANG (23344153) - Briana DAVIES-MORRELL (22734723)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Introduction

The dataset chosen is the Australia's Data Science Job Listings for August 2022, which can be downloaded from [Kaggle](https://www.kaggle.com/datasets/nadzmiagthomas/australia-data-science-jobs). It was scrapped from [Glassdoor](https://www.glassdoor.com.au) which is a website where current and former employees anonymously review companies.

# Setup

Clean and set workspace

```{r}
rm(list = ls())
if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0('package:', names(sessionInfo()$otherPkgs)),
           detach, character.only=TRUE, unload=TRUE))}
set.seed(1)
```

Load libraries and data.

```{r, message=FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(grid)
library(gridExtra)
library(ROCit)
library(caret)
library(lime)
library(knitr)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)
```

# Data Preparation

Firstly, clean the dataset and remove or impute missing values.

```{r}
df[df == ""] <- NA

df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)

# Change the location Australia to Unknown as all the other locations are
# cities other than countries.
df$Job.Location[df$Job.Location == "Australia"] <- "Unknown"

# Drop columns
df <- df[!names(df) %in% c(
    "Country", # always Australia
    "Job.Descriptions", # text based
    "Url", # similar to an ID
    "Company.Founded", # dependent on Company which does not provide new info
    "Company.CEO.Approval", # too many missing values
    "Number.of.Rater" # not valuable
)]

# Drop rows that contain NAs in either Job Title or Company Size
df <- df[!is.na(df$Job.Title) & !is.na(df$Company.Size), ]

# Convert _yn columns to logical
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})

# Convert NAs to Unknown in Company Sector and Company Industry
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})

# Impute NAs with the median value in the rating columns
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- median(!na)
  column
})

str(df)
```

# Classification

## Target Variable

Sometimes job postings do not include the salary. A classification model that predicts the salary range of future job postings, based on past job postings with salary information, would be incredibly useful for job hunters who prioritise salary when applying for jobs. As such, the response variable we have chosen is the income level of the job posting.

The original dataset included three numeric salary variables: `High.Estimate`, `Estimate.Base.Salary`, and `Low.Estimate`. In job descriptions the salary package is often provided as a range rather than a fixed number as there is room for negotiation, the lower end of the range is `Low.Estimate` and the higher end is `High.Estimate`, `Estimate.Base.Salary` is the average of the two. Hence focusing on the `Estimate.Base.Salary` this can be a binary classification problem of whether the income salary is high. The cutoff between these two classes is the median of the `Estimate.Base.Salary`, \$95,000, so one class has a salary between \$0 - \$95,000 and is assigned a label of `FALSE`, the other is a salary between \$95,000 - \$Inf and is assigned a label of `TRUE`. Since the cutoff is based on the median, the dataset is almost balanced, with 955 `TRUE` labels and 950 `FALSE` labels. 

```{r}
target <- "High.Income"
df[, target] <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)

paste(
  "There are",
  nrow(df[df[, target], ]),
  "high income observations. There are",
  nrow(df[!df[, target], ]),
  "low income observations"
)
```

## Feature Variables

Identify the numerical and categorical feature variables.

```{r}
features <- setdiff(colnames(df), target)

df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
categorical_variables <- features[
  sapply(df[, features], class) %in% c("factor", "logical")
]
numerical_variables <- features[
  sapply(df[, features], class) %in% "numeric"
]

paste(
  "There are",
  length(categorical_variables),
  "categorical features,",
  length(numerical_variables),
  "numerical features",
  "and 1 target column."
)
```

Convert numerical variables to categorical.

```{r}
numerical_to_categorical <- c()
for (variable in numerical_variables) {
  categorical_variable <- paste(variable, "categorical", sep = "_")
  numerical_to_categorical <- c(numerical_to_categorical, categorical_variable)
  if (variable == "Friend.Recommendation") {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 100, 10)
    )
  } else {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 5, 0.5)
    )
  }
}
```

## Test and Training Sets

Perform a 80/20 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
test_set <- df[split >= 0.8, ]
training_set <- df[split < 0.8, ]

paste(
  "The training set has",
  nrow(training_set),
  "observations and the test set has",
  nrow(test_set)
)
```

## Null model

The null model will always return the the majority category. As mentioned earlier, the dataset is almost balanced so the model will have a 50% chance of predicting `High.Income (TRUE)`, resulting in an Area Under the Curve (AUC) of 0.5.

```{r}
calc_auc <- function(pred_col, out_col) {
  round(as.numeric(performance(prediction(pred_col, out_col), "auc")@y.values), 3)
}

calc_log_likelihood <- function(ypred, ytrue) {
  ypred <- ypred[ypred > 0 & ypred < 1]
  round(sum(ifelse(ytrue, log(ypred), log(1 - ypred))))
}

null_model <- sum(training_set[target]) / nrow(training_set)
model_evaluations <- data.frame(
  Model.Name = "Null Model",
  Model.Type = "univariate",
  AUC = calc_auc(rep(null_model, nrow(test_set)), test_set[target]),
  Log.Likelihood = calc_log_likelihood(null_model, test_set[, target])
)
kable(model_evaluations)
```

## Single Variable Model

```{r}
single_variable_prediction <- function(pred_col, output_col, test_col) {
  t <- table(pred_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}
```

100-fold cross-validation.

```{r}
cross_validation_100_fold <- function(variable) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(training_set), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      training_set[split, variable],
      training_set[split, target],
      training_set[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, training_set[!split, target])
  }
  mean(aucs)
}
```

Find the average AUC for each variable over 100 fold cross validation and print out the ones over 0.65.

```{r}
single_variable_models <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(single_variable_models) <- c("Variable", "AUC")
for (variable in c(numerical_to_categorical, categorical_variables)) {
  auc <- cross_validation_100_fold(variable)
  single_variable_models[nrow(single_variable_models) + 1, ] <-
    c(variable, auc)

  training_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      training_set[, variable]
    )
  test_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      test_set[, variable]
    )
}

single_variable_models <-
  single_variable_models[
    single_variable_models$AUC > 0.6,
  ]
selected_features <- single_variable_models$Variable

selected_models <-
  single_variable_models[
    order(single_variable_models$AUC, decreasing = TRUE),
  ]
row.names(selected_models) <- NULL

kable(selected_models)
```

Pick the top 2 highest average AUC.

```{r}
company_pred <- single_variable_prediction(
  training_set$Company,
  training_set[, target],
  test_set$Company
)
model_evaluations[nrow(model_evaluations) + 1, ] <- c(
  "Company",
  "univariate",
  calc_auc(company_pred, test_set[target]),
  calc_log_likelihood(company_pred, test_set[, target])
)

title_pred <- single_variable_prediction(
  training_set$Job.Title,
  training_set[, target],
  test_set$Job.Title
)
model_evaluations[nrow(model_evaluations) + 1, ] <- c(
  "Job.Title",
  "univariate",
  calc_auc(title_pred, test_set[target]),
  calc_log_likelihood(title_pred, test_set[, target])
)
```

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Income = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Income)) +
    labs(x = paste("Predicated Probability of", x), y = y)
}
grid.arrange(
  double_density_plot(
    company_pred,
    test_set[target],
    "Company",
    "Density"
  ),
  double_density_plot(
    title_pred,
    test_set[target],
    "Job Title",
    ""
  ),
  ncol = 2
)
```

```{r, fig.width=5, fig.height=5}
roc_plot <- function(pred_col, out_col, colour = "red", overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour, "black"),
    legend = FALSE,
    YIndex = FALSE
  )
}

roc_plot(company_pred, test_set[, target], "orange")
roc_plot(title_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("orange", "blue"),
  c("Job Title", "Company"),
  lwd = 2
)
```

## Model Evaluation

```{r}
lime_plot <- function(model, features, pred) {
  test_cases <- c()

  # True Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # True Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  example <- test_set[test_cases, features]
  explainer <- lime(
    training_set[features],
    model = model,
    bin_continuous = TRUE,
    n_bins = 10
  )

  explanation <- explain(
    example,
    explainer,
    n_labels = 1,
    n_features = length(features)
  )
  plot_features(explanation)
}
```

```{r}
evaluate_model <- function(model, features, name) {
  pred <- predict(
    model,
    test_set[features],
    "prob"
  )[2]
  pred <- unlist(pred)

  auc <- calc_auc(pred, test_set[target])
  log_likelihood <- calc_log_likelihood(pred, test_set[, target])
  print(paste("AUC:", auc))
  print(paste("Log Likelihood:", log_likelihood))

  model_evaluations[nrow(model_evaluations) + 1, ] <- c(
    name,
    "multivariate",
    auc,
    log_likelihood
  )
  assign("model_evaluations", model_evaluations, envir = .GlobalEnv)

  plot(double_density_plot(
    pred,
    test_set[target],
    name,
    "Density"
  ))
  pred
}

make_pred <- function(model, features) {
  pred <- predict(
    model,
    test_set[features],
  )
  as.logical(pred)
}
```

## Naive Bayes

```{r, warning=FALSE}
naive_bayes <- caret::train(
  x = training_set[selected_features],
  y = as.factor(training_set[, target]),
  method = "nb"
)
```

```{r, warning=FALSE, fig.width=5, fig.height=3}
naive_bayes_pred <- evaluate_model(
  naive_bayes,
  selected_features,
  "Naive Bayes"
)
```

```{r, warning=FALSE, fig.width=12, fig.height=8}
pred <- make_pred(naive_bayes, selected_features)
kable(table(test_set[, target], pred))
lime_plot(naive_bayes, selected_features, pred)
```

## Logistic Regression

Logistic regression can be used to classify a variable dependent on one or more independent features. It will find the best fitting model to describe the relationship between the dependent and the independent variables. As it is a binary classification task `binomial` distribution is used.

```{r, warning=FALSE}
probability_columns <- paste(selected_features, "pred", sep = "_")
logistic_regression <- caret::train(
  x = training_set[probability_columns],
  y = as.factor(training_set[, target]),
  method = "glm",
  family = binomial(link = "logit")
)
```

```{r, warning=FALSE, fig.width=5, fig.height=3}
logistic_regression_pred <- evaluate_model(
  logistic_regression,
  probability_columns,
  "Logistic Regression"
)
```

```{r, warning=FALSE, fig.width=12, fig.height=8}
pred <- make_pred(logistic_regression, probability_columns)
kable(table(test_set[, target], pred))
lime_plot(logistic_regression, probability_columns, pred)
```

The top 2 highest performing single model variables `Job.Title` and `Company` are also indicated as the 2 most significant variables shown under the *Coefficients* part of the summary as their p-value are much smaller than others.

```{r}
summary(logistic_regression)
```

## Comparison

```{r, fig.width=5, fig.height=5}
kable(model_evaluations)

roc_plot(naive_bayes_pred, test_set[, target], "orange")
roc_plot(logistic_regression_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("orange", "blue"),
  c("Naive Bayes", "Logistic Regression"),
  lwd = 2
)
```

# Clustering