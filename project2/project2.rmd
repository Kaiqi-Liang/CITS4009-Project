---
title: "CITS4009 Computational Data Analysis"
subtitle: "Project 2 - Modelling"

graphics: yes
author: <i>Kaiqi LIANG (23344153) - Briana DAVIES-MORRELL (22734723)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Introduction

The dataset chosen is the Australia's Data Science Job Listings for August 2022, which can be downloaded from [Kaggle](https://www.kaggle.com/datasets/nadzmiagthomas/australia-data-science-jobs). It was scrapped from [Glassdoor](https://www.glassdoor.com.au) which is a website where current and former employees anonymously review companies.

# Setup

Clean and set workspace

```{r}
rm(list = ls())
if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0('package:', names(sessionInfo()$otherPkgs)),
           detach, character.only=TRUE, unload=TRUE))}
set.seed(1)
```

Load libraries and data.

```{r, message=FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(grid)
library(gridExtra)
library(ROCit)
library(caret)
library(lime)
library(knitr)
library(grDevices)
library(tidyverse)
library(fpc)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)
```

# Data Preparation

Firstly, clean the dataset and remove or impute missing values.

```{r}
df[df == ""] <- NA

df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)

# Change the location Australia to Unknown as all the other locations are
# cities other than countries.
df$Job.Location[df$Job.Location == "Australia"] <- "Unknown"

# Drop columns
df <- df[!names(df) %in% c(
    "Country", # always Australia
    "Job.Descriptions", # text based
    "Url", # similar to an ID
    "Company.Founded", # dependent on Company which does not provide new info
    "Company.CEO.Approval", # too many missing values
    "Number.of.Rater" # not valuable
)]

# Drop rows that contain NAs in either Job.Title or Company.Size
df <- df[!is.na(df$Job.Title) & !is.na(df$Company.Size), ]

# Convert _yn columns to logical
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})

# Convert NAs to Unknown in Company Sector and Company Industry
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})

# Impute NAs with the median value in the rating columns
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- median(!na)
  column
})

str(df)
```

# Classification

## Target Variable

Sometimes job postings do not include the salary. A classification model that predicts the salary range of future job postings, based on past job postings with salary information, would be incredibly useful for job hunters who prioritise salary when applying for jobs. As such, the response variable we have chosen is the salary level of the job posting.

The original dataset included three numeric salary variables: `High.Estimate`, `Estimate.Base.Salary`, and `Low.Estimate`. In job descriptions the salary package is often provided as a range rather than a fixed number as there is room for negotiation, the lower end of the range is `Low.Estimate` and the higher end is `High.Estimate`, `Estimate.Base.Salary` is the average of the two. Hence focusing on the `Estimate.Base.Salary` this can be a binary classification problem of whether the salary is high. The cutoff between these two classes is the median of the `Estimate.Base.Salary`, \$95,000, so one class has a salary between \$0 - \$95,000 and is assigned a label of `FALSE`, the other is a salary between \$95,000 - \$Inf and is assigned a label of `TRUE`. Since the cutoff is based on the median, the dataset is almost balanced, with 955 `TRUE` labels and 950 `FALSE` labels. 

```{r}
target <- "High.Salary"
df[, target] <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)

paste(
  "There are",
  nrow(df[df[, target], ]),
  "high salary observations. There are",
  nrow(df[!df[, target], ]),
  "low salary observations"
)
```

## Feature Variables

Identify the numerical and categorical feature variables.

```{r}
features <- setdiff(colnames(df), target)

df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
categorical_variables <- features[
  sapply(df[, features], class) %in% c("factor", "logical")
]
numerical_variables <- features[
  sapply(df[, features], class) %in% "numeric"
]

paste(
  "There are",
  length(categorical_variables),
  "categorical features,",
  length(numerical_variables),
  "numerical features",
  "and 1 target column."
)
```

Convert numerical variables to categorical so that they can be used to create single variable models.

```{r}
numerical_to_categorical <- c()
for (variable in numerical_variables) {
  categorical_variable <- paste(variable, "categorical", sep = "_")
  numerical_to_categorical <- c(numerical_to_categorical, categorical_variable)
  if (variable == "Friend.Recommendation") {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 100, 10)
    )
  } else {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 5, 0.5)
    )
  }
}
```

## Test and Training Sets

Perform a 80/20 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
test_set <- df[split >= 0.8, ]
training_set <- df[split < 0.8, ]

paste(
  "The training set has",
  nrow(training_set),
  "observations and the test set has",
  nrow(test_set)
)
```

## Null model

The null model will always return the the majority category. As mentioned earlier, the dataset is almost balanced so the model will have a 50% chance of predicting `High.Salary (TRUE)`, resulting in an Area Under the Curve (AUC) of 0.5.

```{r}
calc_auc <- function(pred, ground_truth) {
  round(as.numeric(
    performance(prediction(pred, ground_truth), "auc")@y.values
  ), 4)
}

calc_log_likelihood <- function(pred, ground_truth) {
  pred <- pred[pred > 0 & pred < 1]
  round(sum(ifelse(ground_truth, log(pred), log(1 - pred))))
}

null_model <- sum(training_set[target]) / nrow(training_set)
```

Create a data frame to store the AUC and Log Likelihood of different models.

```{r}
model_evaluations <- data.frame(
  Model.Name = "Null Model",
  Model.Type = "univariate",
  AUC = calc_auc(rep(null_model, nrow(test_set)), test_set[target]),
  Log.Likelihood = calc_log_likelihood(null_model, test_set[, target])
)
kable(model_evaluations)

calc_auc_log_likelihood <- function(pred, name, type) {
  auc <- calc_auc(pred, test_set[target])
  log_likelihood <- calc_log_likelihood(pred, test_set[, target])
  print(paste("AUC:", auc))
  print(paste("Log Likelihood:", log_likelihood))

  model_evaluations[nrow(model_evaluations) + 1, ] <- c(
    name,
    type,
    auc,
    log_likelihood
  )
  assign("model_evaluations", model_evaluations, envir = .GlobalEnv)
}
```

## Single Variable Model

Single variable model based on categorical variables.

```{r}
single_variable_prediction <- function(pred_col, output_col, test_col) {
  t <- table(pred_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}

cross_validation_100_fold <- function(variable) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(training_set), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      training_set[split, variable],
      training_set[split, target],
      training_set[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, training_set[!split, target])
  }
  mean(aucs)
}
```

Find the average AUC for each variable over 100 fold cross validation and save the predicted probabilities back to the data frame so that they can be used in Logistic Regression.

```{r}
single_variable_models <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(single_variable_models) <- c("Variable", "AUC")
for (variable in c(numerical_to_categorical, categorical_variables)) {
  auc <- cross_validation_100_fold(variable)
  single_variable_models[nrow(single_variable_models) + 1, ] <-
    c(variable, auc)

  training_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      training_set[, variable]
    )

  test_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      test_set[, variable]
    )
}
```

Select the variables with an average AUC higher than 0.6 to be the selected features used in training models later on.

```{r}
single_variable_models <-
  single_variable_models[
    single_variable_models$AUC > 0.6,
  ]
selected_features <- single_variable_models$Variable

selected_models <-
  single_variable_models[
    order(single_variable_models$AUC, decreasing = TRUE),
  ]
row.names(selected_models) <- NULL

kable(selected_models)
```

Pick the top 2 single variable models based on their average AUC and calculate the AUC and Log Likelihood using the test set.

```{r}
company_pred <- single_variable_prediction(
  training_set$Company,
  training_set[, target],
  test_set$Company
)
calc_auc_log_likelihood(company_pred, "Company", "univariate")

job_title_pred <- single_variable_prediction(
  training_set$Job.Title,
  training_set[, target],
  test_set$Job.Title
)
calc_auc_log_likelihood(job_title_pred, "Job Title", "univariate")
```

The metrics measured on the test set are higher than the validation set because the latter was averaged across 100 fold.

Plot their predicted probabilities next to each other.

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Salary = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Salary)) +
    labs(x = paste("Predicated Probability of", x), y = y)
}

grid.arrange(
  double_density_plot(
    company_pred,
    test_set[target],
    "Company",
    "Density"
  ),
  double_density_plot(
    job_title_pred,
    test_set[target],
    "Job Title",
    ""
  ),
  ncol = 2
)
```

Compare the ROC curves by plotting them on top of each other.

```{r, fig.width=5, fig.height=5}
roc_plot <- function(pred_col, out_col, colour = "red", overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour, "black"),
    legend = FALSE,
    YIndex = FALSE
  )
}

roc_plot(company_pred, test_set[, target], "orange")
roc_plot(job_title_pred, test_set[, target], "blue", TRUE)
legend(
  "bottomright",
  col = c("orange", "blue"),
  c("Company", "Job Title"),
  lwd = 2
)
```

As shown above using AUC `Company` performs moderately better than `Job.Title` however using Log Likelihood `Job.Title` is better. This means `Company` has a higher performance averaged across all possible decision thresholds but `Job.Title` gives a higher certainty in its predictions as Log Likelihood measures how close the predicted probabilities are to the ground truth (0 or 1).

## Model Evaluation

Functions to call on a model to make predictions, calculate the AUC and Log Likelihood, evaluate which features play a key role in determining if a job posting offers high salary or low salary.

```{r}
confusion_matrix_accuracy <- function(model, features) {
  pred <- as.logical(predict(
    model,
    test_set[features],
  ))

  confusion_matrix <- table(
    ifelse(test_set[, target], "High Salary", "Low Salary"),
    pred
  )[, 2:1]

  print(paste(
    "Precision:",
    format(confusion_matrix[1, 1] / sum(confusion_matrix[, 1]), digits = 3)
  ))

  print(paste(
    "Recall:",
    format(confusion_matrix[1, 1] / sum(confusion_matrix[1, ]), digits = 3)
  ))

  print(kable(confusion_matrix))
  pred
}

evaluate_model <- function(model, features, name) {
  pred <- predict(
    model,
    test_set[features],
    "prob"
  )[2]
  pred <- unlist(pred)

  calc_auc_log_likelihood(pred, name, "multivariate")

  plot(double_density_plot(
    pred,
    test_set[target],
    name,
    "Density"
  ))
  pred
}

lime_plot <- function(model, features, pred) {
  # Pick 4 examples for LIME to explain
  test_cases <- c()

  # True Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Positive
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Negative
  for (i in seq(length(pred))) {
    if (test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # True Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  example <- test_set[test_cases, features]
  explainer <- lime(
    training_set[features],
    model = model,
    bin_continuous = TRUE,
    n_bins = 10
  )

  explanation <- explain(
    example,
    explainer,
    n_labels = 1,
    n_features = length(features)
  )
  plot_features(explanation)
}
```

The LIME plot will have 4 test cases, the top left plot is a True Positive instance, top right is a False Positive instance, bottom left is a False Negative instance and bottom right is a True Negative instance.

## Naive Bayes

Naive Bayes classifier works well on categorical variables which is why it is chosen for this problem as there are many categorical variables with many levels.

```{r, warning=FALSE}
naive_bayes <- caret::train(
  x = training_set[selected_features],
  y = as.factor(training_set[, target]),
  method = "nb"
)
```

Naive Bayes has an AUC of 0.9667 and a Log Likelihood of -116 which slightly outperforms the highest single variable model using the variable `Company` with an AUC of 0.904 and a Log Likelihood of -401.

```{r, warning=FALSE, fig.width=5, fig.height=3}
naive_bayes_pred <- evaluate_model(
  naive_bayes,
  selected_features,
  "Naive Bayes"
)
```

The number of False Positive and False Negative cases are the same which results in the same precision and recall.

```{r, warning=FALSE}
pred <- confusion_matrix_accuracy(naive_bayes, selected_features)
```

For all 4 test cases the number 1 determine factor is either `Company` or `Job.Title` which are the top 2 highest performing single variable models.

```{r, warning=FALSE, fig.width=12, fig.height=8}
lime_plot(naive_bayes, selected_features, pred)
```

## Logistic Regression

Logistic Regression can be used to classify a variable dependent on one or more independent features. It will find the best fitting model to describe the relationship between the dependent and the independent variables. As it is a binary classification task `binomial` distribution is used.

As every categorical variable is expanded to a set of indicator variables in Logistic Regression, it does not work well with large number of levels. Therefore instead of using the original data whose categorical variables contain many levels, the predicted probabilities from single variable models will be used.

```{r, warning=FALSE}
probability_columns <- paste(selected_features, "pred", sep = "_")
logistic_regression <- caret::train(
  x = training_set[probability_columns],
  y = as.factor(training_set[, target]),
  method = "glm",
  family = binomial(link = "logit")
)
```

Logistic Regression has an AUC of 0.9672 and a Log Likelihood of -120 which can be said to have the same performace as Naive Bayes.

```{r, warning=FALSE, fig.width=5, fig.height=3}
logistic_regression_pred <- evaluate_model(
  logistic_regression,
  probability_columns,
  "Logistic Regression"
)
```

Logistic Regression has a lower precision but higher recall than Naive Bayes which means Logistic Regression is able to identify more of the high salary jobs but makes more mistakes while doing so.

```{r}
pred <- confusion_matrix_accuracy(logistic_regression, probability_columns)
```

Except for the False Negative case shown by the bottom left LIME plot the predicted probabilities from the `Company` single varible model is the most supporting feature which again aligns with the highest single variable model.

```{r, warning=FALSE, fig.width=12, fig.height=8}
lime_plot(logistic_regression, probability_columns, pred)
```

The top 2 highest performing single model variables `Company` and `Job.Title` are also indicated as the 2 most significant variables shown under the *Coefficients* part of the summary as their p-value are much smaller than others.

```{r}
summary(logistic_regression)
```

## Comparison
```{r, fig.width=5, fig.height=5}
kable(model_evaluations)

roc_plot(naive_bayes_pred, test_set[, target], "orange")
roc_plot(logistic_regression_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("orange", "blue"),
  c("Naive Bayes", "Logistic Regression"),
  lwd = 2
)
```

# Clustering 

The goal of clustering is to discover similarities among subsets of our data. Given we have demonstrated a binary classification, it will interesting to see the dataset forms clusters around the salary of the job posting. To do so we will investigate the clustering of numerical and integer variables, that is the variables reporting on the job pay and job rating.

```{r}
num_var <- colnames(df[
  sapply(df, class) %in% c("numeric", "integer")])

head(df[,num_var])
```

We standardize the variables to ensure the disparity in scale does not adversely affect clustering:

```{r}
scaled_df <- scale(df[,num_var])
head(scaled_df)
```

## Hierarchical Clustering

Here we perform hierarchical clustering to explore possible splits in our data. While we previously performed a binary classification, we wish to explore all possible partitions of our data, and thus chose Hierarchical over K-Means. We use the distance metric of euclidean distance, and investigate four popular different linkage methods; Single, Complete, Average, and Ward.D2. For each method, a dendrogram, illustrating the nested clusters, is plotted. 

```{r}

# Firstly, calculate the distance between each observation
dist_matrix <-  dist(scaled_df, method = "euclidean")

# Next, perform hierarchical cluster on the distance matrix trying different linkage methods. 
# Plot the dendrogram for each linkage

par(mfrow=c(2,2))

# Single Linkage 
clust_single <-  hclust(dist_matrix, method <- "single")
plot(clust_single, labels=FALSE, main="Single Linkage")

# Complete Linkage
clust_complete <-  hclust(dist_matrix, method <- "complete")
plot(clust_complete, labels=FALSE, main="Complete Linkage") 

# Average Linkage
clust_average <-  hclust(dist_matrix, method <- "average")
plot(clust_average, labels=FALSE, main="Average Linkage")

# Ward D2 Linkage 
clust_wardD2 <-  hclust(dist_matrix, method <- "ward.D2")
plot(clust_wardD2,labels=FALSE, main="Ward.D2 Linkage")

```

As evident above, the linkage method has a significant impact on the type of clusters formed. The single linkage is best for detecting outliers, but does not perform well for a large number of observations and levels. Complete measures the distance between the two most distance points, and therefore produces tighter clusters. Average linkage is similar to complete linkage but also assists in detection of outliers. Finally, Ward.D2 minimizes the within cluster variance by combining clusters according to the smallest between cluster distance, and therefore produce compact clusters that are easily differential. For the purposes of this assignment Ward.D2 will be used.

We can cut the Ward.D2 dendrogram at different heights to return the cluster sizes. Here we show the distribution of observations for 1-9 clusters 

```{r}

# Cutree gives clusters at different cluster levels in a table 
dend_classes9 = cutree(clust_wardD2, k = 1:9) 

# Number of observations in each class
dend_classes9.fp = as.data.frame( dend_classes9 ) %>%
  pivot_longer(cols = 1:9,
               names_to = "max_clust",
               values_to = "cluster"
               )

conf = xtabs( ~ cluster + max_clust, dend_classes9.fp )
conf

```

The table above indicates the distribution of observations at cluster levels 1-9. The dataset has 1905 observations, and this splits into 1751 and 154 at a cluster level of two. At the third cluster level the dataset is split into 1410, 341, and 154 observations. This indicates the majority of the data demonstrates similiar characteristics, except for 154 observations which are significantly different. 

### Optimal Number of Clusters

To determine the optimal number of clusters for our dataset we should consider the total Within Sum of Squares (WSS) and the Calinski-Harabasz index (CH index). An optimal number of clusters is defined such that WSS is minimized and the CH-Index is maximized; that is, there is limited variance within clusters and large variance between clusters.  

The functions below are used to calculate the WSS and CH-Index for different cluster levels. 

```{r}

# Function to return the squared Euclidean distance of two given points x and y
sqr_euDist <- function(x, y) {
  sum((x - y)^2)
}

# Function to calculate WSS of a cluster
wss <- function(clustermat) {
  c0 <- colMeans(clustermat)
  sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}

# Function to calculate the total WSS
wss_total <- function(scaled_df, labels) {
  wss.sum <- 0
  k <- length(unique(labels))
  for (i in 1:k)
    wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
  wss.sum
}

# Function to calculate total sum of squared (TSS) distance of data
tss <- function(scaled_df) {
  wss(scaled_df)
}

# Function to return the CH indices computed using hierarchical
# clustering (function `hclust`) or k-means clustering (`kmeans`)
CH_index <- function(scaled_df, kmax, method="kmeans") {
  if (!(method %in% c("kmeans", "hclust")))
  stop("method must be one of c('kmeans', 'hclust')")
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
  # wss.value[1] stores the WSS value for k=1 (when all the
  # data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  if (method == "kmeans") {
  # kmeans
  for (k in 2:kmax) {
  clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
  wss.value[k] <- clustering$tot.withinss
  }
  } else {
  # hclust
  d <- dist(scaled_df, method="euclidean")
  pfit <- hclust(d, method="ward.D2")
  for (k in 2:kmax) {
  labels <- cutree(pfit, k=k)
  wss.value[k] <- wss_total(scaled_df, labels)
  }
  }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax-1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
```

Plot the CH-Index and WSS across different k values to visualize the optimal cluster number.

```{r}
# calculate the CH criterion
crit.df <- CH_index(scaled_df, 10, method="hclust")

fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
  geom_point() + 
  geom_line(colour="red") +
  scale_x_continuous(breaks=1:10, labels=1:10) +
  labs(y="CH index") + 
  theme(text=element_text(size=20))

fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
  geom_point() + geom_line(colour="blue") +
  scale_x_continuous(breaks=1:10, labels=1:10) +
  theme(text=element_text(size=20))

grid.arrange(fig1, fig2, nrow=1)
```

As evident in the graph above, the data's CH index is maximized at 2 clusters, whereas the WSS is minimized at 10. We usually consider the 'elbow', the point at which the rate of change of WSS slows down, to be a reasonable estimate of the optimal number of clusters. For the graph above, the elbow appears to be at 4-5. This corresponds to an 'almost' local maximum in the CH-index. Thus, five clusters seem to be a good choice to maximize the distance between clusters and minimize the variability within clusters.  

Moreover, we can visualize the distribution of observations within the clusters to confirm if 5 clusters is a good choice.

```{r}

PCA <- prcomp(scaled_df)
nComp <- 2 
project2D <- as.data.frame(predict(PCA, newdata=scaled_df)[,1:nComp])

find_convex_hull <- function(proj2Ddf, clusters) {
  do.call(rbind,
    lapply(unique(clusters),
      FUN = function(c) {
        f <- subset(proj2Ddf, cluster==c);
        f[chull(f),]
      }
      )
    )
  }

fig <- c()
kvalues <- seq(2,5)
for (k in kvalues) {
  clusters <- cutree(clust_wardD2, k)
  hclust.project2D <- cbind(project2D, cluster=as.factor(clusters), salary=df$Estimate.Base.Salary)
  hclust.hull <- find_convex_hull(hclust.project2D, clusters)
  assign(paste0("fig", k),
    ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
      geom_point(aes(shape=cluster, color=cluster, alpha=0.2)) +
      geom_polygon(data=hclust.hull, aes(group=cluster, fill=cluster),
      alpha=0.4, linetype=0) +
      labs(title = sprintf("k = %d", k)) +
      theme(legend.position="none", text=element_text(size=20))
  )
}
grid.arrange(fig2, fig3, fig4, fig5, nrow=2, top = textGrob("Cluster Distribution for k=2-5",gp=gpar(fontsize=20,font=3)))
```
As evident in the figure above, the data is first split into two distinct clusters, one of which (red) demonstrates significant variability. 

### Validating our Clusters

We should assess if five clusters represent true variations in our dataset. Clusterboot() uses boostrap sampling to evaluate how stable a given cluster is. 

```{r}
n_clust <- 5
cboot.hclust <-  clusterboot(scaled_df, clustermethod = hclustCBI, method="ward.D2", k=n_clust)
clusters.cboot <- cboot.hclust$result$partition
```

To determine stability, consider how many times the four clusters were dissolved during clusterboot. The code below returns 1 - % of times, and therefore a number close to 1 indicates high stability whereas a lower number indicates low stability 

```{r}
cluster_number <- c(1,2,3,4,5)
stability <- c(values <- 1 - cboot.hclust$bootbrd/100) 

clus_stab <- data.frame(cluster_number,stability)
clus_stab
```

As evident in the table above, all clusters 4 and 5 are stable with stability values of 1 and 0.85 respectively. Cluster 2, on the other hand, is not stable.

### Exploring the Clusters 

From here on out we work with five clusters to explore the patterns they represent in our data. First append the cluster number to the original dataset.  

```{r}
# Cut the dendrogram to form 5 clusters
n_clust = 5
clusters <- cutree(clust_wardD2, n_clust)
df$cluster <- as.factor(clusters)
```

We will first consider the locations of the jobs from each cluster.

```{r}
ggplot(df, aes(cluster, fill=State)) + 
  geom_bar(position="fill")
```

As seen in the filled bar chart above, there is no distinct pattern to be found between the clusters and job location. 

Next we can consider the salary, and see if the clusters form around this. 

```{r}
ggplot(df, aes(Estimate.Base.Salary, fill=cluster)) + 
  geom_histogram()
```
Cluster 1 is the largest and covers salaries less than \$108,000. Cluster 5 predominantly covers salaries greater than than \$108,000 but less than \$170,000. Cluster 3 covers the high-end salaries greater \$170,000. There is some overlap between these three segments and clusters 2 and 4, but for the most part the figure above demonstrates a separation of data into 'high', 'middle', and 'low' salaries. The lack of a clear pattern for cluster 2 makes sense, given it's low stability of 0.33, but it is interesting that cluster 4, which had a stability of 1, does not demonstrate any clear relationship with the salary. 

As such, we investigate if other numerical variables used in classification, the ratings, have any relationships with the clusters formed. The code below plots the Company.Rating vs. Estimate.Base.Salary by cluster group. 

```{r}
ggplot(df, aes(Company.Rating, Estimate.Base.Salary, colour = factor( cluster ) ) ) + geom_point()
```

As evident in the plot above,while cluster 4 is spread out over many possible salaries, it is actually reflective of a very low company rating, mainly a rating of 1. Cluster 2 is situated around higher company ratings, but as demonstrated through clusterboot(), this is not reflective of a true variation in the dataset. 
