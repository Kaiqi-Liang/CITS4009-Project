---
title: "CITS4009 Computational Data Analysis"
subtitle: "Project 2 - Modelling"

graphics: yes
author: <i>Kaiqi LIANG (23344153) - Briana DAVIES-MORRELL (22734723)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Introduction

The dataset chosen is the Australia's Data Science Job Listings for August 2022, which can be downloaded from [Kaggle](https://www.kaggle.com/datasets/nadzmiagthomas/australia-data-science-jobs). It was scrapped from [Glassdoor](https://www.glassdoor.com.au) which is a website where current and former employees anonymously review companies.

# Setup

Clean and set workspace

```{r}
rm(list = ls())
if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0('package:', names(sessionInfo()$otherPkgs)),
           detach, character.only=TRUE, unload=TRUE))}
```

Load libraries and dataset as df.

```{r, message=FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(grid)
library(gridExtra)
library(ROCit)
library(caret)
library(lime)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)
```

# Data Preparation

Firstly, clean the dataset and remove or impute missing values. This is the same process as undertaken in Project 1 and is resultingly not explained here. 

```{r, echo=FALSE}
df[df == ""] <- NA

df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)

# Change the location Australia to Unknown as all the other locations are cities other than countries.
df$Job.Location[df$Job.Location == "Australia"] <- "Unknown"

# Drop columns
# Country is always Australia so not a valuable variable
# Job Descriptions and URL are text based and outside the scope of this unit
# Company.Founded is dependent on Company, doesn't provide any new information for the classification model
# Company.CEO.Approval has too many missing values
# Number.of.Rater is the number of people in the company who rated - not a valuable variab;e 
df <- df[!names(df) %in% c(
    "Country",
    "Job.Descriptions",
    "Url",
    "Company.Founded",
    "Company.CEO.Approval",
    "Number.of.Rater"
)]

# Drop rows that contain NAs in either Job Title or Company Size
df <- df[!is.na(df$Job.Title) & !is.na(df$Company.Size), ]

# Convert _yn columns to logical
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})

# Convert NAs to Unknown in Company Sector and Company Industry
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})

# Impute NAs with median values in the rating columns
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- median(!na)
  column
})

str(df)
```

# Classification

## Target Variable

Sometimes job postings don't include the salary. A classification model that predicts the salary range of future job postings, based on past postings with salary information, would be incredibly useful for job hunters who don't want to waste time applying for unsuitable jobs. As such, the response variable we have chosen is the income level of the job posting. The original dataset included three numeric salary variables: High.Estimate, Estimate.Base.Salary, and Low.Estimate. Focusing on the Estimate.Base.Salary this can be a binary classification problem of two classes: High.Income and Low.Income. We have taken the cutoff between these two classes to be the median of the Estimate.Base.Salary, \$95,000. Low.Income is a salary between \$0 - \$95,000 and is assigned a label of 0. High.Income is a salary between \$95,000 - \$Inf and is assigned a label of 1. Since the cutoff is based on the median, the dataset is almost balanced, with 955 High.Income observations and 950 Low.Income observations. 

Create the "target" column High.Income by splitting the Estimate.Base.Salary at the median value of \$95,000. As we are approaching this classification problem as a solution to a lack of salary information, drop the three numeric salary variables: High.Estimate, Estimate.Base.Salary, and Low.Estimate. 

```{r}
target <- "High.Income"
df[, target] <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)
df <- df[!names(df) %in% c(
  "High.Estimate",
  "Estimate.Base.Salary",
  "Low.Estimate"
)]
paste(
  "There are",
  nrow(df[df[, target], ]),
  "high income observations. There are",
  nrow(df[!df[, target], ]),
  "low income observations",
  sep = " "
)
```

## Feature Variables

Next we need to select the feature variables that will act as predictors of the target variable, High.Income. All variables left after data cleaning could potentially influence the job posting salary. We are looking for independent variables that could affect salary. 

Company.Sector is dependent on Company.Industry. Job.Location is dependent on State. Possibly only one of these is needed, this will be clear in the classification problem.

Identify the numerical and categorical feature variables:

```{r}
features <- setdiff(colnames(df), target)

categorical_variables <- features[
  sapply(df[, features], class) %in% c("factor", "character", "logical")
]
numerical_variables <- features[
  sapply(df[, features], class) %in% "numeric"
]

paste(
  "There are",
  length(categorical_variables),
  "categorical features,",
  length(numerical_variables),
  "numerical features",
  "and 1 target column.",
  sep = " "
)
```

Convert numerical variables to categorical.

```{r}
numerical_to_categorical <- c()
for (variable in numerical_variables) {
  categorical_variable <- paste(variable, "categorical", sep = "_")
  numerical_to_categorical <- c(numerical_to_categorical, categorical_variable)
  if (variable == "Friend.Recommendation") {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 100, 10)
    )
  } else {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 5, 0.5)
    )
  }
}
```

## Test and Training Sets 

Perform a 90/10 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
test_set <- df[split >= 0.9, ]
training_set <- df[split < 0.9, ]

paste(
  "The training set has",
  nrow(training_set),
  "observations and the test set has",
  nrow(test_set),
  sep = " "
)
```

## Null model

Firstly, we will compute a null model. The null model will always return the the majority category. As mentioned earlier, the dataset is almost balanced so the model will have a 50% chance of predicting High.Income (`TRUE`), resulting in an Area Under the Curve (AUC) of 0.5. 

Calc_auc is a formula that calculates the AUC of a model based on the predictor and target. 

```{r}
calc_auc <- function(pred_col, out_col) {
  as.numeric(performance(prediction(pred_col, out_col), "auc")@y.values)
}
log_likelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1 - ypred)))
}
```

```{r}
null_model <- sum(training_set[target]) / nrow(training_set)
(calc_auc(rep(null_model, nrow(test_set)), test_set[target]))
(log_likelihood(null_model, training_set[, target]))
```

## Single Variable Model

```{r}
single_variable_prediction <- function(pred_col, output_col, test_col) {
  t <- table(pred_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}
```

100-fold cross-validation.

```{r}
cross_validation_100_fold <- function(variable) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(training_set), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      training_set[split, variable],
      training_set[split, target],
      training_set[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, training_set[!split, target])
  }
  mean_auc <- mean(aucs)
  if (mean_auc > 0.6) {
    print(sprintf("%s: %4.3f", variable, mean_auc))
  }
  mean_auc
}
```

Find the average auc for each variable over 100 fold cross validation and print out the ones over 0.65.

```{r}
for (variable in numerical_to_categorical) {
  cross_validation_100_fold(variable)
}
for (variable in categorical_variables) {
  cross_validation_100_fold(variable)
}
```

Pick the top 2 highest average AUC.

```{r}
industry_pred <- single_variable_prediction(
  training_set$Company.Industry,
  training_set[, target],
  test_set$Company.Industry
)
recommendation_pred <- single_variable_prediction(
  training_set$Friend.Recommendation,
  training_set[, target],
  test_set$Friend.Recommendation
)
```

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Income = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Income)) +
    labs(x = x, y = y)
}
grid.arrange(
  double_density_plot(
    industry_pred,
    test_set[target],
    "Company Industry",
    "density"
  ),
  double_density_plot(
    recommendation_pred,
    test_set[target],
    "Friend Recommendation",
    ""
  ),
  ncol = 2
)
```

```{r, fig.asp=1, out.width="50%"}
roc_plot <- function(pred_col, out_col, colour = "red", overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour, "black"),
    legend = FALSE,
    YIndex = FALSE
  )
  legend(
    "bottomright",
    col = c("orange", "blue"),
    c("Job Title", "Company"),
    lwd = 2
  )
}
roc_plot(industry_pred, test_set[, target], "orange")
roc_plot(recommendation_pred, test_set[, target], "blue", TRUE)
```

```{r}
plot_lime <- function(model, features) {
  explainer <- lime(
    training_set[features],
    model = model,
    bin_continuous = TRUE,
    n_bins = 10
  )
  example <- test_set[c(1, 3), features]
  explanation <- explain(example, explainer, n_labels = 1, n_features = 4)
  plot_features(explanation)
}
```

## Naive Bayes

```{r}
naive_bayes <- caret::train(
  x = training_set[features],
  y = as.factor(training_set[, target]),
  method = "nb"
)
```

```{r}
evaluate_model <- function(model, features) {
  pred <- predict(
    model,
    test_set[features],
    "prob"
  )[2]
  pred <- unlist(pred)
  print(calc_auc(pred, test_set[target]))
  roc_plot(pred, test_set[, target])
  double_density_plot(
    pred,
    test_set[target],
    "Naive Bayes",
    "density"
  )
  plot_lime(model, features)
}
```

```{r}
evaluate_model(naive_bayes, features)
```

```{r}
(f <- paste(
  "High.Income ~",
  paste(
    c(
      categorical_variables,
      paste(numerical_variables, "_categorical", sep = "")
    ),
    collapse = " + "
  )
))
model <- naiveBayes(as.formula(f), training_set)
```

## Logistic Regression

Logistic regression can be used to classify a variable dependent on one or more independent features. LR will find the best fitting model to describe the relationship between the dependent and the independent variables. We will be using binomial type LR, as the target variable can only have two possible types - High.Income or not. 

```{r}
logistic_variables <- setdiff(colnames(df), c(
  "Company",
  "Job.Title",
  "Company.Industry",
  "Job.Location",
  "Company.Type"
))
logistic_regression <- caret::train(
  x = training_set[logistic_variables],
  y = as.factor(training_set[, target]),
  method = "glm",
  family = binomial(link = "logit")
)
```

Firstly, create the formula for logistic regression and fit the model
```{r}

# fmla <- paste(target, paste(features, collapse=" + "), sep=" ~ ")
# cat(fmla)

model <- glm(High.Income~., data=training_set, family=binomial(link="logit"))
# summary(model)
# predict(logistic, newData = test_set[features])
# plot_lime(logistic, features)
summary(model)
```

```{r}
evaluate_model(logistic_regression, logistic_variables)
```

Next use the model to make predictions on the training and test set. 

The predict function gives the probability that the observation has a response of 1 - i.e. is a High.Income observations. 

Overlaying these probabilities reveal that the model has a 100% chance of predicting 1 for High.Income observations and 0 for Low.Income observations.
```{r}

train$pred <- predict(model, data = train, type="response")

ggplot(train, aes(x = pred)) +
  geom_histogram(aes(color = High.Income, fill = High.Income), position = "identity", bins = 50, alpha = 0.4) + 
  ggtitle('Probability of Predicting the Observation is High.Income') + 
  xlab('Probability')

train %>% select(pred, High.Income)

```


```{r}

# Prediction, precision, and recall objects
prediction_obj <- prediction(train$pred, train[target])
precision_obj <- performance(perf, measure="prec")
recall_obj <- performance(perf, measure="rec")

thresh <- (precision_obj@x.values)[[1]] # threshold
precision <- (precision_obj@y.values)[[1]] # precision
recall <- (recall_obj@y.values)[[1]] # recall

ROCdf <- data.frame(threshold=thresh, precision=precision, recall=recall)
ROCdf

# Null Probability 
pnull <- mean(as.numeric(train[target]))
cat('pnull=', pnull)
```

Plot the enrichment rate (the ratio of precision to the average rate of positives) and recall versus the threshold value

```{r}
p1 <- ggplot(ROCdf, aes(x=threshold)) + geom_line(aes(y=precision/pnull)) +
coord_cartesian(xlim = c(0,1), ylim=c(0,5) ) + labs(y="Enrichment rate")
p2 <- ggplot(ROCdf, aes(x=threshold)) + geom_line(aes(y=recall)) +
coord_cartesian(xlim = c(0,1))
grid.arrange(p1, p2, nrow = 2)
```
For test set 

Ran into a problem. Job.Title has some unique instances, e.g. Animal Health Advisor. Thus, it is possible for the trained model to miss these (if they were randomly assigned into test). We cannot predict for which we have no data.

For now try removing these cases. This is almost definitely wrong. Need to fix cause the graphs look wacky.

```{r}
test_modified <- test
test_modified$Job.Title[which(!(test_modified$Job.Title %in% unique(train$Job.Title)))] <- NA  # Replace new levels by NA
test_modified$Job.Location[which(!(test_modified$Job.Location %in% unique(train$Job.Location)))] <- NA  # Replace new levels by NA
test_modified$Company[which(!(test_modified$Company %in% unique(train$Company)))] <- NA  # Replace new levels by NA
test_modified$Company.Industry[which(!(test_modified$Company.Industry %in% unique(train$Company.Industry)))] <- NA  # Replace new levels by NA

```

```{r}

test$pred <- predict(model, newdata = test_modified, type="response")

ggplot(test, aes(x = pred)) +
  geom_histogram(aes(color = High.Income, fill = High.Income), position = "identity", bins = 50, alpha = 0.4) + 
  ggtitle('Probability of Predicting the Observation is High.Income') + 
  xlab('Probability')

test_modified %>% select(pred, High.Income)

```
```{r}
cat("Confusion matrix of 'High.Income' predictions:\n")

## Confusion matrix of 'High.Income' predictions:
confusion.test <- table(actual=test[target], predicted=test$pred>0.05)
confusion.test

precision <- confusion.test[2,2] / sum(confusion.test[,2]) # TP / (TP+FP)

recall <- confusion.test[2,2] / sum(confusion.test[2,]) # TP / (TP+FN)

enrich <- precision / mean(as.numeric(test[target]))

print(paste("The precision is", as.character(precision), ". The recall is", as.character(recall), "and the enrichment rate is", as.character(enrich), sep = " "))

```


# Clustering 
