---
title: "CITS4009 Computational Data Analysis"
subtitle: "Project 2 - Modelling"

graphics: yes
author: <i>Kaiqi LIANG (23344153) - Briana DAVIES-MORRELL (22734723)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Data Preparation

```{r}
# Clean and set workspace 
rm(list = ls())
if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0('package:', names(sessionInfo()$otherPkgs)),
           detach, character.only=TRUE, unload=TRUE))}
```

```{r, message=FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(gridExtra)
library(ROCit)
library(caret)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)
```

```{r, echo=FALSE}
df[df == ""] <- NA

df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)

# Drop columns
df <- df[!names(df) %in% c(
    "Country",
    "Job.Descriptions",
    "Url",
    "Company.Founded",
    "Company.CEO.Approval",
    "Number.of.Rater"
)]

# Drop rows that contain NAs in either Job Title or Company Size
df <- df[!is.na(df$Job.Title) & !is.na(df$Company.Size), ]

# Convert _yn columns to logical
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})

# Convert NAs to Unknown in Company Sector and Company Industry
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})

# Impute NAs with median values in the rating columns
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- median(!na)
  column
})
```

Create target column by splitting a numerical variable into 2 categories using the median.

```{r}
df$High.Income <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)
df <- df[!names(df) %in% c(
  "High.Estimate",
  "Estimate.Base.Salary",
  "Low.Estimate"
)]
```

Perform a 90/10 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
test <- df[split >= 0.9, ]
train <- df[split < 0.9, ]
```

# Classification

## Null model
```{r}
calc_auc <- function(pred_col, out_col) {
  as.numeric(performance(prediction(pred_col, out_col), "auc")@y.values)
}
```

As it is a balanced dataset the null model will have a 50% chance of predicting `TRUE` which results in an AUC of 0.5.

```{r}
null_model <- sum(train$High.Income) / nrow(train)
calc_auc(rep(null_model, nrow(test)), test$High.Income)
```

## Single Variable Model
```{r}
variables <- setdiff(colnames(df), "High.Income")
categorical_variables <- variables[
  sapply(train[, variables], class) %in% c("factor", "character", "logical")
]
numeric_variables <- variables[
  sapply(train[, variables], class) %in% "numeric"
]
```

```{r}
single_variable_prediction <- function(feature_col, output_col, test_col) {
  t <- table(feature_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}
```

100-fold cross-validation.

```{r}
for (variable in categorical_variables) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(train), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      train[split, variable],
      train[split, "High.Income"],
      train[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, train[!split, "High.Income"])
  }
  mean_auc <- mean(aucs)
  if (mean_auc > 0.65) {
    print(sprintf("%s: %4.3f", variable, mean_auc))
  }
}
```

Pick the top 2 highest average AUC.

```{r}
title_pred <- single_variable_prediction(
  train$Job.Title,
  train$High.Income,
  test$Job.Title
)
company_pred <- single_variable_prediction(
  train$Company,
  train$High.Income,
  test$Company
)
```

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Income = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Income)) +
    labs(x = x, y = y)
}
grid.arrange(
  double_density_plot(
    title_pred,
    test$High.Income,
    "Job Title",
    "density"
  ),
  double_density_plot(
    company_pred,
    test$High.Income,
    "Company",
    ""
  ),
  ncol = 2
)
```

```{r, out.width="50%"}
roc_plot <- function(pred_col, out_col, colour_id = 2, overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour_id, 1),
    legend = FALSE,
    YIndex = FALSE
  )
}
roc_plot(title_pred, test$High.Income)
roc_plot(company_pred, test$High.Income, 3, TRUE)
```

## Naive Bayes

## Logistic Regression

Create formula for logistic regression.

```{r}

predictor = "High.Income"
features = colnames(df)[colnames(df) != "High.Income"]

fmla <- paste(predictor, paste(features, collapse=" + "), sep=" ~ ")
cat(fmla)

```

Fit the logistic regression model

```{r}

model <- glm(fmla, data=train, family=binomial(link="logit"))

summary(model)
```

Make predictions. Return the probabilities of the predictor

```{r}

train$pred <- predict(model, newdata=train, type="response")
test$pred <- predict(model, newdata=test, type="response")

```
Ran into a problem. Job.Title has some unique instances, e.g. Animal Health Advisor. Thus, it is possible for the trained model to miss these (if they were randomly assigned into test). We cannot predict for which we have no data.

For now try removing these cases. This is almost definitely wrong. Need to fix cause the graphs look wacky.

```{r}
test_modified <- test
test_modified$Job.Title[which(!(test_modified$Job.Title %in% unique(train$Job.Title)))] <- NA  # Replace new levels by NA
test_modified$Job.Location[which(!(test_modified$Job.Location %in% unique(train$Job.Location)))] <- NA  # Replace new levels by NA
test_modified$Company[which(!(test_modified$Company %in% unique(train$Company)))] <- NA  # Replace new levels by NA
test_modified$Company.Industry[which(!(test_modified$Company.Industry %in% unique(train$Company.Industry)))] <- NA  # Replace new levels by NA

```

```{r}

train$pred <- predict(model, newdata=train, type="response")
test$pred <- predict(model, newdata=test_modified, type="response")

```

```{r}

ggplot(train, aes(x=pred, color=High.Income, linetype=High.Income)) + geom_density(size=1.5) +theme(text=element_text(size=20))

```

```{r}

perf <- prediction(train$pred, train$High.Income)
precObj <- performance(perf, measure="prec")
recObj <- performance(perf, measure="rec")
thresh <- (precObj@x.values)[[1]] # threshold
precision <- (precObj@y.values)[[1]] # precision
recall <- (recObj@y.values)[[1]] # recall
ROCdf <- data.frame(threshold=thresh, precision=precision, recall=recall)

# Null 
pnull <- mean(as.numeric(train$High.Income))

```

```{r}
p1 <- ggplot(ROCdf, aes(x=threshold)) + geom_line(aes(y=precision/pnull)) +
coord_cartesian(xlim = c(0,0.05), ylim=c(0,5) ) + labs(y="Enrichment rate")
p2 <- ggplot(ROCdf, aes(x=threshold)) + geom_line(aes(y=recall)) +
coord_cartesian(xlim = c(0,0.05))
grid.arrange(p1, p2, nrow = 2)
```

