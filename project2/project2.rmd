---
title: "CITS4009 Computational Data Analysis"
subtitle: "Project 2 - Modelling"

graphics: yes
author: <i>Kaiqi LIANG (23344153) - Briana DAVIES-MORRELL (22734723)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Introduction

The dataset chosen is the Australia's Data Science Job Listings for August 2022, which can be downloaded from [Kaggle](https://www.kaggle.com/datasets/nadzmiagthomas/australia-data-science-jobs). It was scrapped from [Glassdoor](https://www.glassdoor.com.au) which is a website where current and former employees anonymously review companies.

# Setup

Clean and set workspace

```{r}
rm(list = ls())
if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0('package:', names(sessionInfo()$otherPkgs)),
           detach, character.only=TRUE, unload=TRUE))}
set.seed(1)
```

Load libraries and data.

```{r, message=FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(grid)
library(gridExtra)
library(ROCit)
library(caret)
library(lime)
library(knitr)
library(grDevices)
library(tidyverse)
library(fpc)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)
```

# Data Preparation

Firstly, clean the dataset and remove or impute missing values.

```{r}
df[df == ""] <- NA

df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)

# Change the location Australia to Unknown as all the other locations are
# cities other than countries.
df$Job.Location[df$Job.Location == "Australia"] <- "Unknown"

# Drop columns
df <- df[!names(df) %in% c(
    "Country", # always Australia
    "Job.Descriptions", # text based
    "Url", # similar to an ID
    "Company.Founded", # dependent on Company which does not provide new info
    "Company.CEO.Approval", # too many missing values
    "Number.of.Rater" # not valuable
)]

# Drop rows that contain NAs in either Job Title or Company Size
df <- df[!is.na(df$Job.Title) & !is.na(df$Company.Size), ]

# Convert _yn columns to logical
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})

# Convert NAs to Unknown in Company Sector and Company Industry
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})

# Impute NAs with the median value in the rating columns
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- median(!na)
  column
})

str(df)
```

# Classification

## Target Variable

Sometimes job postings do not include the salary. A classification model that predicts the salary range of future job postings, based on past job postings with salary information, would be incredibly useful for job hunters who prioritise salary when applying for jobs. As such, the response variable we have chosen is the income level of the job posting.

The original dataset included three numeric salary variables: `High.Estimate`, `Estimate.Base.Salary`, and `Low.Estimate`. In job descriptions the salary package is often provided as a range rather than a fixed number as there is room for negotiation, the lower end of the range is `Low.Estimate` and the higher end is `High.Estimate`, `Estimate.Base.Salary` is the average of the two. Hence focusing on the `Estimate.Base.Salary` this can be a binary classification problem of whether the income salary is high. The cutoff between these two classes is the median of the `Estimate.Base.Salary`, \$95,000, so one class has a salary between \$0 - \$95,000 and is assigned a label of `FALSE`, the other is a salary between \$95,000 - \$Inf and is assigned a label of `TRUE`. Since the cutoff is based on the median, the dataset is almost balanced, with 955 `TRUE` labels and 950 `FALSE` labels. 

```{r}
target <- "High.Income"
df[, target] <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)

paste(
  "There are",
  nrow(df[df[, target], ]),
  "high income observations. There are",
  nrow(df[!df[, target], ]),
  "low income observations"
)
```

## Feature Variables

Identify the numerical and categorical feature variables.

```{r}
features <- setdiff(colnames(df), target)

df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
categorical_variables <- features[
  sapply(df[, features], class) %in% c("factor", "logical")
]
numerical_variables <- features[
  sapply(df[, features], class) %in% "numeric"
]

paste(
  "There are",
  length(categorical_variables),
  "categorical features,",
  length(numerical_variables),
  "numerical features",
  "and 1 target column."
)
```

Convert numerical variables to categorical.

```{r}
numerical_to_categorical <- c()
for (variable in numerical_variables) {
  categorical_variable <- paste(variable, "categorical", sep = "_")
  numerical_to_categorical <- c(numerical_to_categorical, categorical_variable)
  if (variable == "Friend.Recommendation") {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 100, 10)
    )
  } else {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 5, 0.5)
    )
  }
}
```

## Test and Training Sets

Perform a 80/20 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
test_set <- df[split >= 0.8, ]
training_set <- df[split < 0.8, ]

paste(
  "The training set has",
  nrow(training_set),
  "observations and the test set has",
  nrow(test_set)
)
```

## Null model

The null model will always return the the majority category. As mentioned earlier, the dataset is almost balanced so the model will have a 50% chance of predicting `High.Income (TRUE)`, resulting in an Area Under the Curve (AUC) of 0.5.

```{r}
calc_auc <- function(pred_col, out_col) {
  round(as.numeric(performance(prediction(pred_col, out_col), "auc")@y.values), 3)
}

calc_log_likelihood <- function(ypred, ytrue) {
  ypred <- ypred[ypred > 0 & ypred < 1]
  round(sum(ifelse(ytrue, log(ypred), log(1 - ypred))))
}

null_model <- sum(training_set[target]) / nrow(training_set)
model_evaluations <- data.frame(
  Model.Name = "Null Model",
  Model.Type = "univariate",
  AUC = calc_auc(rep(null_model, nrow(test_set)), test_set[target]),
  Log.Likelihood = calc_log_likelihood(null_model, test_set[, target])
)
kable(model_evaluations)
```

## Single Variable Model

```{r}
single_variable_prediction <- function(pred_col, output_col, test_col) {
  t <- table(pred_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}
```

100-fold cross-validation.

```{r}
cross_validation_100_fold <- function(variable) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(training_set), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      training_set[split, variable],
      training_set[split, target],
      training_set[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, training_set[!split, target])
  }
  mean(aucs)
}
```

Find the average AUC for each variable over 100 fold cross validation and print out the ones over 0.65.

```{r}
single_variable_models <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(single_variable_models) <- c("Variable", "AUC")
for (variable in c(numerical_to_categorical, categorical_variables)) {
  auc <- cross_validation_100_fold(variable)
  single_variable_models[nrow(single_variable_models) + 1, ] <-
    c(variable, auc)

  training_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      training_set[, variable]
    )
  test_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      test_set[, variable]
    )
}

single_variable_models <-
  single_variable_models[
    single_variable_models$AUC > 0.6,
  ]
selected_features <- single_variable_models$Variable

selected_models <-
  single_variable_models[
    order(single_variable_models$AUC, decreasing = TRUE),
  ]
row.names(selected_models) <- NULL

kable(selected_models)
```

Pick the top 2 highest average AUC.

```{r}
company_pred <- single_variable_prediction(
  training_set$Company,
  training_set[, target],
  test_set$Company
)
model_evaluations[nrow(model_evaluations) + 1, ] <- c(
  "Company",
  "univariate",
  calc_auc(company_pred, test_set[target]),
  calc_log_likelihood(company_pred, test_set[, target])
)

title_pred <- single_variable_prediction(
  training_set$Job.Title,
  training_set[, target],
  test_set$Job.Title
)
model_evaluations[nrow(model_evaluations) + 1, ] <- c(
  "Job.Title",
  "univariate",
  calc_auc(title_pred, test_set[target]),
  calc_log_likelihood(title_pred, test_set[, target])
)
```

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Income = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Income)) +
    labs(x = paste("Predicated Probability of", x), y = y)
}
grid.arrange(
  double_density_plot(
    company_pred,
    test_set[target],
    "Company",
    "Density"
  ),
  double_density_plot(
    title_pred,
    test_set[target],
    "Job Title",
    ""
  ),
  ncol = 2
)
```

```{r, fig.width=5, fig.height=5}
roc_plot <- function(pred_col, out_col, colour = "red", overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour, "black"),
    legend = FALSE,
    YIndex = FALSE
  )
}

roc_plot(company_pred, test_set[, target], "orange")
roc_plot(title_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("orange", "blue"),
  c("Job Title", "Company"),
  lwd = 2
)
```

## Model Evaluation

```{r}
lime_plot <- function(model, features, pred) {
  test_cases <- c()

  # True Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # True Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  example <- test_set[test_cases, features]
  explainer <- lime(
    training_set[features],
    model = model,
    bin_continuous = TRUE,
    n_bins = 10
  )

  explanation <- explain(
    example,
    explainer,
    n_labels = 1,
    n_features = length(features)
  )
  plot_features(explanation)
}
```

```{r}
evaluate_model <- function(model, features, name) {
  pred <- predict(
    model,
    test_set[features],
    "prob"
  )[2]
  pred <- unlist(pred)

  auc <- calc_auc(pred, test_set[target])
  log_likelihood <- calc_log_likelihood(pred, test_set[, target])
  print(paste("AUC:", auc))
  print(paste("Log Likelihood:", log_likelihood))

  model_evaluations[nrow(model_evaluations) + 1, ] <- c(
    name,
    "multivariate",
    auc,
    log_likelihood
  )
  assign("model_evaluations", model_evaluations, envir = .GlobalEnv)

  plot(double_density_plot(
    pred,
    test_set[target],
    name,
    "Density"
  ))
  pred
}

make_pred <- function(model, features) {
  pred <- predict(
    model,
    test_set[features],
  )
  as.logical(pred)
}
```

## Naive Bayes

```{r, warning=FALSE}
naive_bayes <- caret::train(
  x = training_set[selected_features],
  y = as.factor(training_set[, target]),
  method = "nb"
)
```

```{r, warning=FALSE, fig.width=5, fig.height=3}
naive_bayes_pred <- evaluate_model(
  naive_bayes,
  selected_features,
  "Naive Bayes"
)
```

```{r, warning=FALSE, fig.width=12, fig.height=8}
pred <- make_pred(naive_bayes, selected_features)
kable(table(test_set[, target], pred))
lime_plot(naive_bayes, selected_features, pred)
```

## Logistic Regression

Logistic regression can be used to classify a variable dependent on one or more independent features. It will find the best fitting model to describe the relationship between the dependent and the independent variables. As it is a binary classification task `binomial` distribution is used.

```{r, warning=FALSE}
probability_columns <- paste(selected_features, "pred", sep = "_")
logistic_regression <- caret::train(
  x = training_set[probability_columns],
  y = as.factor(training_set[, target]),
  method = "glm",
  family = binomial(link = "logit")
)
```

```{r, warning=FALSE, fig.width=5, fig.height=3}
logistic_regression_pred <- evaluate_model(
  logistic_regression,
  probability_columns,
  "Logistic Regression"
)
```

```{r, warning=FALSE, fig.width=12, fig.height=8}
pred <- make_pred(logistic_regression, probability_columns)
kable(table(test_set[, target], pred))
lime_plot(logistic_regression, probability_columns, pred)
```

The top 2 highest performing single model variables `Job.Title` and `Company` are also indicated as the 2 most significant variables shown under the *Coefficients* part of the summary as their p-value are much smaller than others.

```{r}
summary(logistic_regression)
```

## Comparison
```{r, fig.width=5, fig.height=5}
kable(model_evaluations)

roc_plot(naive_bayes_pred, test_set[, target], "orange")
roc_plot(logistic_regression_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("orange", "blue"),
  c("Naive Bayes", "Logistic Regression"),
  lwd = 2
)
```

# Clustering 

The goal of clustering is to discover similarities among subsets of the data. 
Set the dataframe to be used in clustering. We are investigating the clustering of numerical and integer variables, that is the variables reporting on the job pay and ratings.

```{r}

clust_df = df
head(clust_df)

num_var <- colnames(df[
  sapply(df, class) %in% c("numeric", "integer")
])

```

Next, standardize the variables to ensure the disparity in scale does not adversely affect clustering 

```{r}
scaled_df <- scale(clust_df[,num_var])
head(scaled_df)
```

## Hierarchial Clustering

Here we perform hierarchial clustering to explore possible splits in our data. We use the distance metric of euclidean distance, and investigate six different linkage methods; Single, Centroid, Complete, Average, Ward.D, and Ward.D2. For each method, a dendrogram, illustrating the nested clusters, is plotted. 

```{r}

# Fristly, calculate the distance between each observation
dist_matrix <-  dist(scaled_df, method = "euclidean")

# Next, perform hierarchical cluster on the distance matrix trying different linkage methods. 
# Plot the dendrogram for each linkage

par(mfrow=c(2,3))

# Single Linkage 
clust_single <-  hclust(dist_matrix, method <- "single")
plot(clust_single, labels=FALSE, main="Single Linkage")

# Centroid Linkage 
clust_cent <-  hclust(dist_matrix, method <- "centroid")
plot(clust_cent,labels=FALSE, main="Centroid Linkage")

# Complete Linkage
clust_complete <-  hclust(dist_matrix, method <- "complete")
plot(clust_complete, labels=FALSE, main="Complete Linkage") 

# Average Linkage
clust_average <-  hclust(dist_matrix, method <- "average")
plot(clust_average, labels=FALSE, main="Average Linkage")

# Ward D Linkage
clust_wardD<-  hclust(dist_matrix, method <- "ward.D")
plot(clust_wardD,labels=FALSE, main="Ward.D Linkage")

# Ward D2 Linkage 
clust_wardD2 <-  hclust(dist_matrix, method <- "ward.D2")
plot(clust_wardD2,labels=FALSE, main="Ward.D2 Linkage")

```
As evident above, the linkage method has a significant impact on the type of clusters formed. The single linkage is best for detecting outliers, but does not perform well for a large number of observations and levels. Complete measures the distance between the two most distance points, and therefore produces tighter clusters. Average linkage is similar to complete linkage but also assists in detection of outliers. Finally, both Ward.D and Ward.D2 minimize the within cluster variance by combining clusters according to the smallest between cluster distance, and therefor produce compact clusters. In particular, Ward.D2 squares the differences identified in Ward.D, thereby making clusters easier to differentiate. For the purposes of this assignment Ward.D2 will be used.

We can cut the Ward.D2 dendrogram at different heights to return the cluster sizes. Here we show the distribution of observations for 1-9 clusters 

```{r}

# Cutree gives clusters at different cluster levels in a table 
dend_classes = cutree(clust_wardD, k = 1:9) 

# Number of observations in each class
dend_classes9.fp = as.data.frame( dend_classes9 ) %>%
  pivot_longer(cols = 1:9,
               names_to = "max_clust",
               values_to = "cluster"
               )

conf = xtabs( ~ cluster + max_clust, dend_classes9.fp )
conf

```

The table above indicates the distribution of observations at cluster levels 1-9. The dataset has 1905 observations, and this splits into 1751 and 154 at a cluster level of two. At the third cluster level the dataset is split into 1410, 341, and 154 observations. This indicates the majority of the data demonstrates similiar characteristics, except for 154 observations which are significantly different. 

### Optimal Number of Clusters

To determine the optimal number of clusters for our dataset we should consider the total Within Sum of Squares (WSS) and the Calinski-Harabasz index (CH index). An optimal number of clusters is defined such that WSS is minimized and the CH-Index is maximized; that is, there is limited variance within clusters and large variance between clusters.  

The functions below are used to calculate the WSS and CH-Index for different cluster levels. 
```{r}

# Function to return the squared Euclidean distance of two given points x and y
sqr_euDist <- function(x, y) {
  sum((x - y)^2)
}

# Function to calculate WSS of a cluster
wss <- function(clustermat) {
  c0 <- colMeans(clustermat)
  sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}

# Function to calculate the total WSS
wss_total <- function(scaled_df, labels) {
  wss.sum <- 0
  k <- length(unique(labels))
  for (i in 1:k)
    wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
  wss.sum
}

# Function to calculate total sum of squared (TSS) distance of data
tss <- function(scaled_df) {
  wss(scaled_df)
}

# Function to return the CH indices computed using hierarchical
# clustering (function `hclust`) or k-means clustering (`kmeans`)
CH_index <- function(scaled_df, kmax, method="kmeans") {
  if (!(method %in% c("kmeans", "hclust")))
  stop("method must be one of c('kmeans', 'hclust')")
  npts <- nrow(scaled_df)
  wss.value <- numeric(kmax) # create a vector of numeric type
  # wss.value[1] stores the WSS value for k=1 (when all the
  # data points form 1 large cluster).
  wss.value[1] <- wss(scaled_df)
  if (method == "kmeans") {
  # kmeans
  for (k in 2:kmax) {
  clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
  wss.value[k] <- clustering$tot.withinss
  }
  } else {
  # hclust
  d <- dist(scaled_df, method="euclidean")
  pfit <- hclust(d, method="ward.D2")
  for (k in 2:kmax) {
  labels <- cutree(pfit, k=k)
  wss.value[k] <- wss_total(scaled_df, labels)
  }
  }
  bss.value <- tss(scaled_df) - wss.value # this is a vector
  B <- bss.value / (0:(kmax-1)) # also a vector
  W <- wss.value / (npts - 1:kmax) # also a vector
  data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
```

Plot the CH-Index and WSS across different k values to visualize the optimal cluster number.
```{r}
# calculate the CH criterion
crit.df <- CH_index(scaled_df, 10, method="hclust")

fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
  geom_point() + 
  geom_line(colour="red") +
  scale_x_continuous(breaks=1:10, labels=1:10) +
  labs(y="CH index") + 
  theme(text=element_text(size=20))

fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
  geom_point() + geom_line(colour="blue") +
  scale_x_continuous(breaks=1:10, labels=1:10) +
  theme(text=element_text(size=20))

grid.arrange(fig1, fig2, nrow=1)
```
As evident in the graph above, the data's CH index is maximised at 2 clusters, whereas the WSS is minimized at 10. We usually consider the 'elbow', the point at which the rate of change of WSS slows down, to be a reasonable estimate as for the optimal number of clusters. For the graph above, the elbow appears to be at 4-5. This corresponds to a 'almost' local maximum in the CH-index. Thus, five clusters seem to be a good choice to maximise the distance between clusters and minimize the variability within clusters.  

### Validating our Clusters

We should assess if five clusters represent true variations in our dataset. Clusterboot() uses boostrap sampling to evaluate how stable a given cluster is. 

```{r}
n_clust <- 5
cboot.hclust <-  clusterboot(scaled_df, clustermethod = hclustCBI, method="ward.D", k=n_clust)
clusters.cboot <- cboot.hclust$result$partition
```

To determine stability, consider how many times the four clusters were dissolved during clusterboot. The code below returns 1 - % of times, and therefore a number close to 1 indicates high stability whereas a lower number indicates low stability 

```{r}
cluster_number <- c(1,2,3,4,5)
stability <- c(values <- 1 - cboot.hclust$bootbrd/100) 

clus_stab <- data.frame(cluster_number,stability)
clus_stab
```

As evident in the table above, all five clusters are very stable with values very close to 1. Cluster 4, in particular, is very indicative of pattern in our dataset. 

We can additionally analysis the cluster distribution by looking the the mean of each cluster's variables, and by plotting the convex hull. 

```{r}
aggregate(clust_df[,num_var], by=list(cluster=clust_df$cluster), mean)
```

```{r}

PCA <- prcomp(scaled_df)
nComp <- 2 
project2D <- as.data.frame(predict(PCA, newdata=scaled_df)[,1:nComp])

find_convex_hull <- function(proj2Ddf, clusters) {
  do.call(rbind,
    lapply(unique(clusters),
      FUN = function(c) {
        f <- subset(proj2Ddf, cluster==c);
        f[chull(f),]
      }
      )
    )
  }
hclust.hull <- find_convex_hull(hclust.project2D, clusters)

fig <- c()
kvalues <- seq(2,5)
for (k in kvalues) {
  clusters <- cutree(clust_wardD, k)
  hclust.project2D <- cbind(project2D, cluster=as.factor(clusters), salary=df$Estimate.Base.Salary)
  hclust.hull <- find_convex_hull(hclust.project2D, clusters)
  assign(paste0("fig", k),
    ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
      geom_point(aes(shape=cluster, color=cluster, alpha=0.2)) +
      geom_polygon(data=hclust.hull, aes(group=cluster, fill=cluster),
      alpha=0.4, linetype=0) +
      labs(title = sprintf("k = %d", k)) +
      theme(legend.position="none", text=element_text(size=20))
  )
}
grid.arrange(fig2, fig3, fig4, fig5, nrow=2, top = textGrob("Cluster Distribution for k=2-5",gp=gpar(fontsize=20,font=3)))
```
### Exploring the Clusters 

From here on out we work with five clusters to explore the patterns they represent in our data. First append the cluster number to the original dataset.  

```{r}
# Cut the dendrogram to form 5 clusters
n_clust = 5
clusters <- cutree(clust_wardD, n_clust)
clust_df$cluster <- as.factor(clusters)
```

We will first consider the locations of the jobs from each cluster
```{r}
ggplot(clust_df, aes(cluster, fill=State)) + 
  geom_bar(position="fill")
```
As seen in the filled bar chart above, there is no distinct pattern to be found between the clusters and job location. 

Next we can consider the pay, and see if the clusters form around this. 
```{r}
ggplot(clust_df, aes(Estimate.Base.Salary, fill=cluster)) + 
  geom_histogram()
```
Cluster 1 is the largest and covers salaries less than \$126,086. Cluster 2 predominantly covers salaries greater than \$120,000. There is some overlap between the cluster 1 and 2, but for the most part they seperate the data into a 'high' and 'low' salary. However, clusters 3,4,5 overlap greately with cluster 1 with no clear pattern. 

It is also worthwhile plotting the Company.Rating vs. Estimate.Base.Salary by cluster group. 
```{r}
ggplot(clust_df, aes(Company.Rating, Estimate.Base.Salary, colour = factor( cluster ) ) ) + geom_point()
```
This plot clearly demonstrates the split between cluster 4, and the rest of the cluster. Recalling the analysis of stability, cluster 4 recieved a score of 1 which reflects the clear verticaln split in our data. Cluster 2, which was shown to be representive of a high salary is also representative of a high company rating. 



