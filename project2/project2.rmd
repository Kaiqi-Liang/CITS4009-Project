---
title: "CITS4009 Computational Data Analysis"
subtitle: "Project 2 - Modelling"

graphics: yes
author: <i>Kaiqi LIANG (23344153) - Briana DAVIES-MORRELL (22734723)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Introduction

The dataset chosen is the Australia's Data Science Job Listings for August 2022, which can be downloaded from [Kaggle](https://www.kaggle.com/datasets/nadzmiagthomas/australia-data-science-jobs). It was scrapped from [Glassdoor](https://www.glassdoor.com.au) which is a website where current and former employees anonymously review companies.

# Setup

Clean and set workspace

```{r}
rm(list = ls())
if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0('package:', names(sessionInfo()$otherPkgs)),
           detach, character.only=TRUE, unload=TRUE))}
```

Load libraries and dataset as df.

```{r, message=FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(grid)
library(gridExtra)
library(ROCit)
library(caret)
library(lime)
library(knitr)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)
```

# Data Preparation

Firstly, clean the dataset and remove or impute missing values. This is the same process as undertaken in Project 1 and is resultingly not explained here. 

```{r, echo=FALSE}
df[df == ""] <- NA

df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)

# Change the location Australia to Unknown as all the other locations are cities other than countries.
df$Job.Location[df$Job.Location == "Australia"] <- "Unknown"

# Drop columns
# Country is always Australia so not a valuable variable
# Job Descriptions and URL are text based and outside the scope of this unit
# Company.Founded is dependent on Company, doesn't provide any new information for the classification model
# Company.CEO.Approval has too many missing values
# Number.of.Rater is the number of people in the company who rated - not a valuable variab;e 
df <- df[!names(df) %in% c(
    "Country",
    "Job.Descriptions",
    "Url",
    "Company.Founded",
    "Company.CEO.Approval",
    "Number.of.Rater"
)]

# Drop rows that contain NAs in either Job Title or Company Size
df <- df[!is.na(df$Job.Title) & !is.na(df$Company.Size), ]

# Convert _yn columns to logical
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})

# Convert NAs to Unknown in Company Sector and Company Industry
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})

# Impute NAs with median values in the rating columns
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- median(!na)
  column
})

str(df)
```

# Classification

## Target Variable

Sometimes job postings don't include the salary. A classification model that predicts the salary range of future job postings, based on past postings with salary information, would be incredibly useful for job hunters who don't want to waste time applying for unsuitable jobs. As such, the response variable we have chosen is the income level of the job posting. The original dataset included three numeric salary variables: High.Estimate, Estimate.Base.Salary, and Low.Estimate. Focusing on the Estimate.Base.Salary this can be a binary classification problem of two classes: High.Income and Low.Income. We have taken the cutoff between these two classes to be the median of the Estimate.Base.Salary, \$95,000. Low.Income is a salary between \$0 - \$95,000 and is assigned a label of 0. High.Income is a salary between \$95,000 - \$Inf and is assigned a label of 1. Since the cutoff is based on the median, the dataset is almost balanced, with 955 High.Income observations and 950 Low.Income observations. 

Create the "target" column High.Income by splitting the Estimate.Base.Salary at the median value of \$95,000. As we are approaching this classification problem as a solution to a lack of salary information, drop the three numeric salary variables: High.Estimate, Estimate.Base.Salary, and Low.Estimate. 

```{r}
target <- "High.Income"
df[, target] <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)
df <- df[!names(df) %in% c(
  "High.Estimate",
  "Estimate.Base.Salary",
  "Low.Estimate"
)]
paste(
  "There are",
  nrow(df[df[, target], ]),
  "high income observations. There are",
  nrow(df[!df[, target], ]),
  "low income observations",
  sep = " "
)
```

## Feature Variables

Next we need to select the feature variables that will act as predictors of the target variable, High.Income. All variables left after data cleaning could potentially influence the job posting salary. We are looking for independent variables that could affect salary. 

Company.Sector is dependent on Company.Industry. Job.Location is dependent on State. Possibly only one of these is needed, this will be clear in the classification problem.

Identify the numerical and categorical feature variables:

```{r}
features <- setdiff(colnames(df), target)

categorical_variables <- features[
  sapply(df[, features], class) %in% c("factor", "character", "logical")
]
numerical_variables <- features[
  sapply(df[, features], class) %in% "numeric"
]

paste(
  "There are",
  length(categorical_variables),
  "categorical features,",
  length(numerical_variables),
  "numerical features",
  "and 1 target column.",
  sep = " "
)
```

Convert numerical variables to categorical.

```{r}
numerical_to_categorical <- c()
for (variable in numerical_variables) {
  categorical_variable <- paste(variable, "categorical", sep = "_")
  numerical_to_categorical <- c(numerical_to_categorical, categorical_variable)
  if (variable == "Friend.Recommendation") {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 100, 10)
    )
  } else {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 5, 0.5)
    )
  }
}
```

## Test and Training Sets 

Perform a 90/10 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
test_set <- df[split >= 0.9, ]
training_set <- df[split < 0.9, ]

paste(
  "The training set has",
  nrow(training_set),
  "observations and the test set has",
  nrow(test_set),
  sep = " "
)
```

## Null model

Firstly, we will compute a null model. The null model will always return the the majority category. As mentioned earlier, the dataset is almost balanced so the model will have a 50% chance of predicting High.Income (`TRUE`), resulting in an Area Under the Curve (AUC) of 0.5. 

Calc_auc is a formula that calculates the AUC of a model based on the predictor and target. 

```{r}
calc_auc <- function(pred_col, out_col) {
  as.numeric(performance(prediction(pred_col, out_col), "auc")@y.values)
}
log_likelihood <- function(ypred, ytrue) {
  sum(ifelse(ytrue, log(ypred), log(1 - ypred)))
}
```

```{r}
null_model <- sum(training_set[target]) / nrow(training_set)
(calc_auc(rep(null_model, nrow(test_set)), test_set[target]))
(log_likelihood(null_model, training_set[, target]))
```

## Single Variable Model

```{r}
single_variable_prediction <- function(pred_col, output_col, test_col) {
  t <- table(pred_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}
```

100-fold cross-validation.

```{r}
cross_validation_100_fold <- function(variable) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(training_set), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      training_set[split, variable],
      training_set[split, target],
      training_set[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, training_set[!split, target])
  }
  mean_auc <- mean(aucs)
  if (mean_auc > 0.6) {
    print(sprintf("%s: %4.3f", variable, mean_auc))
  }
  mean_auc
}
```

Find the average auc for each variable over 100 fold cross validation and print out the ones over 0.65.

```{r}
for (variable in c(numerical_to_categorical, categorical_variables)) {
  cross_validation_100_fold(variable)
  training_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      training_set[, variable]
    )
  test_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      test_set[, variable]
    )
}
```

Pick the top 2 highest average AUC.

```{r}
company_pred <- single_variable_prediction(
  training_set$Company,
  training_set[, target],
  test_set$Company
)
title_pred <- single_variable_prediction(
  training_set$Job.Title,
  training_set[, target],
  test_set$Job.Title
)
```

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Income = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Income)) +
    labs(x = x, y = y)
}
grid.arrange(
  double_density_plot(
    company_pred,
    test_set[target],
    "Company Industry",
    "density"
  ),
  double_density_plot(
    title_pred,
    test_set[target],
    "Friend Recommendation",
    ""
  ),
  ncol = 2
)
```

```{r, fig.asp=1, out.width="50%"}
roc_plot <- function(pred_col, out_col, colour = "red", overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour, "black"),
    legend = FALSE,
    YIndex = FALSE
  )
  legend(
    "bottomright",
    col = c("orange", "blue"),
    c("Job Title", "Company"),
    lwd = 2
  )
}
roc_plot(company_pred, test_set[, target], "orange")
roc_plot(title_pred, test_set[, target], "blue", TRUE)
```

## Model Evaluation

```{r}
lime_plot <- function(model, features, pred) {
  test_cases <- c()
  for (i in seq(length(pred))) {
    if (test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }
  for (i in seq(length(pred))) {
    if (test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }
  explainer <- lime(
    training_set[features],
    model = model,
    bin_continuous = TRUE,
    n_bins = 10
  )
  example <- test_set[test_cases, features]
  explanation <- explain(example, explainer, n_labels = 1, n_features = length(features))
  plot_features(explanation)
}
```

```{r}
evaluate_model <- function(model, features) {
  pred <- predict(
    model,
    test_set[features],
    "prob"
  )[2]
  pred <- unlist(pred)
  print(calc_auc(pred, test_set[target]))
  roc_plot(pred, test_set[, target])
  double_density_plot(
    pred,
    test_set[target],
    "Naive Bayes",
    "density"
  )
}

make_pred <- function(model, features) {
  pred <- predict(
    model,
    test_set[features],
  )
  as.logical(pred)
}
```

## Naive Bayes

```{r, warning=FALSE}
naive_bayes <- caret::train(
  x = training_set[features],
  y = as.factor(training_set[, target]),
  method = "nb"
)
```

```{r, warning=FALSE, out.width="50%"}
evaluate_model(naive_bayes, features)
```

```{r, warning=FALSE, fig.width=10, fig.height=20}
pred <- make_pred(naive_bayes, features)
kable(table(test_set[, target], pred))
lime_plot(naive_bayes, features, pred)
```

## Logistic Regression

Logistic regression can be used to classify a variable dependent on one or more independent features. LR will find the best fitting model to describe the relationship between the dependent and the independent variables. We will be using binomial type LR, as the target variable can only have two possible types - High.Income or not. 

```{r, warning=FALSE}
probability_columns <- grep("_pred", names(training_set))
logistic_regression <- caret::train(
  x = training_set[probability_columns],
  y = as.factor(training_set[, target]),
  method = "glm",
  family = binomial(link = "logit")
)
```

```{r, warning=FALSE, out.width="50%"}
evaluate_model(logistic_regression, probability_columns)
```

```{r, warning=FALSE, fig.width=10, fig.height=20}
pred <- make_pred(logistic_regression, probability_columns)
kable(table(test_set[, target], pred))
lime_plot(logistic_regression, probability_columns, pred)
```

```{r}
summary(logistic_regression)
```

# Clustering 


Firstly standardize numeric features to ensure scale does not adversely affect clustering 
```{r}
scaled_df <- scale(df[,numerical_variables])
head(scaled_df)
```

Using the distance metric 'euclidean', output a dendrogram - a tree that represents nested clusters
```{r}

dist <-  dist(scaled_df, method = "euclidean")

clust <-  hclust(dist, method <- "ward.D2")
# summary(clust)
# clust$height

plot(clust)
rect.hclust(clust, k=5)
#gives the dendrogram which shows cluster levels 

# clus.den = as.dendrogram( hclust(dist(scaled_df)), method = "euclidean" )
# 
# clus.den = hclust((dist(scaled_df), method = "euclidean" ))
# 
# plot(clus.den)
```
Cutree takes in a clustering model and returns a vector of cluster group assignments for each row. We specify the number of groups by using k = 2

```{r}

groups <- cutree(clust, k=5)
# groups

print_clusters <- function(df, groups, cols_to_print) {
  n_groups <- max(groups)
  for (i in 1:n_groups) {
    print(paste("cluster", i))
    print(df[groups == i, cols_to_print])
  }
}

cols_to_print <- c('Company.Revenue', numerical_variables)
print_clusters(df, groups, cols_to_print)

```

```{r}
df$group <- groups
ggplot(df, aes(groups, State)) + 
  geom_count()
```


```{r}

groups.fp = as.data.frame( groups ) %>%
  pivot_longer(cols = 1,
               names_to = "max_clust",
               values_to = "cluster"
               )
conf = xtabs( ~ cluster + max_clust, groups.fp )
conf

```

