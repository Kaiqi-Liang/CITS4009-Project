---
title: "CITS4009 Computational Data Analysis"
subtitle: "Project 2 - Modelling"

graphics: yes
author: <i>Kaiqi LIANG (23344153) - Briana DAVIES-MORRELL (22734723)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Introduction

The dataset chosen is the Australia's Data Science Job Listings for August 2022, which can be downloaded from [Kaggle](https://www.kaggle.com/datasets/nadzmiagthomas/australia-data-science-jobs). It was scrapped from [Glassdoor](https://www.glassdoor.com.au) which is a website where current and former employees anonymously review companies.

# Setup

Clean and set workspace

```{r}
rm(list = ls())
if (!is.null(sessionInfo()$otherPkgs)) {
  invisible(
    lapply(paste0('package:', names(sessionInfo()$otherPkgs)),
           detach, character.only=TRUE, unload=TRUE))}
set.seed(1)
```

Load libraries and data.

```{r, message=FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(grid)
library(gridExtra)
library(ROCit)
library(caret)
library(lime)
library(knitr)
library(grDevices)
library(tidyverse)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)
```

# Data Preparation

Firstly, clean the dataset and remove or impute missing values.

```{r}
df[df == ""] <- NA

df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)

# Change the location Australia to Unknown as all the other locations are
# cities other than countries.
df$Job.Location[df$Job.Location == "Australia"] <- "Unknown"

# Drop columns
df <- df[!names(df) %in% c(
    "Country", # always Australia
    "Job.Descriptions", # text based
    "Url", # similar to an ID
    "Company.Founded", # dependent on Company which does not provide new info
    "Company.CEO.Approval", # too many missing values
    "Number.of.Rater" # not valuable
)]

# Drop rows that contain NAs in either Job Title or Company Size
df <- df[!is.na(df$Job.Title) & !is.na(df$Company.Size), ]

# Convert _yn columns to logical
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})

# Convert NAs to Unknown in Company Sector and Company Industry
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})

# Impute NAs with the median value in the rating columns
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- median(!na)
  column
})

str(df)
```

# Classification

## Target Variable

Sometimes job postings do not include the salary. A classification model that predicts the salary range of future job postings, based on past job postings with salary information, would be incredibly useful for job hunters who prioritise salary when applying for jobs. As such, the response variable we have chosen is the income level of the job posting.

The original dataset included three numeric salary variables: `High.Estimate`, `Estimate.Base.Salary`, and `Low.Estimate`. In job descriptions the salary package is often provided as a range rather than a fixed number as there is room for negotiation, the lower end of the range is `Low.Estimate` and the higher end is `High.Estimate`, `Estimate.Base.Salary` is the average of the two. Hence focusing on the `Estimate.Base.Salary` this can be a binary classification problem of whether the income salary is high. The cutoff between these two classes is the median of the `Estimate.Base.Salary`, \$95,000, so one class has a salary between \$0 - \$95,000 and is assigned a label of `FALSE`, the other is a salary between \$95,000 - \$Inf and is assigned a label of `TRUE`. Since the cutoff is based on the median, the dataset is almost balanced, with 955 `TRUE` labels and 950 `FALSE` labels. 

```{r}
target <- "High.Income"
df[, target] <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)

paste(
  "There are",
  nrow(df[df[, target], ]),
  "high income observations. There are",
  nrow(df[!df[, target], ]),
  "low income observations"
)
```

## Feature Variables

Identify the numerical and categorical feature variables.

```{r}
features <- setdiff(colnames(df), target)

df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
categorical_variables <- features[
  sapply(df[, features], class) %in% c("factor", "logical")
]
numerical_variables <- features[
  sapply(df[, features], class) %in% "numeric"
]

paste(
  "There are",
  length(categorical_variables),
  "categorical features,",
  length(numerical_variables),
  "numerical features",
  "and 1 target column."
)
```

Convert numerical variables to categorical.

```{r}
numerical_to_categorical <- c()
for (variable in numerical_variables) {
  categorical_variable <- paste(variable, "categorical", sep = "_")
  numerical_to_categorical <- c(numerical_to_categorical, categorical_variable)
  if (variable == "Friend.Recommendation") {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 100, 10)
    )
  } else {
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 5, 0.5)
    )
  }
}
```

## Test and Training Sets

Perform a 80/20 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
test_set <- df[split >= 0.8, ]
training_set <- df[split < 0.8, ]

paste(
  "The training set has",
  nrow(training_set),
  "observations and the test set has",
  nrow(test_set)
)
```

## Null model

The null model will always return the the majority category. As mentioned earlier, the dataset is almost balanced so the model will have a 50% chance of predicting `High.Income (TRUE)`, resulting in an Area Under the Curve (AUC) of 0.5.

```{r}
calc_auc <- function(pred_col, out_col) {
  round(as.numeric(performance(prediction(pred_col, out_col), "auc")@y.values), 3)
}

calc_log_likelihood <- function(ypred, ytrue) {
  ypred <- ypred[ypred > 0 & ypred < 1]
  round(sum(ifelse(ytrue, log(ypred), log(1 - ypred))))
}

null_model <- sum(training_set[target]) / nrow(training_set)
model_evaluations <- data.frame(
  Model.Name = "Null Model",
  Model.Type = "univariate",
  AUC = calc_auc(rep(null_model, nrow(test_set)), test_set[target]),
  Log.Likelihood = calc_log_likelihood(null_model, test_set[, target])
)
kable(model_evaluations)
```

## Single Variable Model

```{r}
single_variable_prediction <- function(pred_col, output_col, test_col) {
  t <- table(pred_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}
```

100-fold cross-validation.

```{r}
cross_validation_100_fold <- function(variable) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(training_set), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      training_set[split, variable],
      training_set[split, target],
      training_set[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, training_set[!split, target])
  }
  mean(aucs)
}
```

Find the average AUC for each variable over 100 fold cross validation and print out the ones over 0.65.

```{r}
single_variable_models <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(single_variable_models) <- c("Variable", "AUC")
for (variable in c(numerical_to_categorical, categorical_variables)) {
  auc <- cross_validation_100_fold(variable)
  single_variable_models[nrow(single_variable_models) + 1, ] <-
    c(variable, auc)

  training_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      training_set[, variable]
    )
  test_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      test_set[, variable]
    )
}

single_variable_models <-
  single_variable_models[
    single_variable_models$AUC > 0.6,
  ]
selected_features <- single_variable_models$Variable

selected_models <-
  single_variable_models[
    order(single_variable_models$AUC, decreasing = TRUE),
  ]
row.names(selected_models) <- NULL

kable(selected_models)
```

Pick the top 2 highest average AUC.

```{r}
company_pred <- single_variable_prediction(
  training_set$Company,
  training_set[, target],
  test_set$Company
)
model_evaluations[nrow(model_evaluations) + 1, ] <- c(
  "Company",
  "univariate",
  calc_auc(company_pred, test_set[target]),
  calc_log_likelihood(company_pred, test_set[, target])
)

title_pred <- single_variable_prediction(
  training_set$Job.Title,
  training_set[, target],
  test_set$Job.Title
)
model_evaluations[nrow(model_evaluations) + 1, ] <- c(
  "Job.Title",
  "univariate",
  calc_auc(title_pred, test_set[target]),
  calc_log_likelihood(title_pred, test_set[, target])
)
```

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Income = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Income)) +
    labs(x = paste("Predicated Probability of", x), y = y)
}
grid.arrange(
  double_density_plot(
    company_pred,
    test_set[target],
    "Company",
    "Density"
  ),
  double_density_plot(
    title_pred,
    test_set[target],
    "Job Title",
    ""
  ),
  ncol = 2
)
```

```{r, fig.width=5, fig.height=5}
roc_plot <- function(pred_col, out_col, colour = "red", overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour, "black"),
    legend = FALSE,
    YIndex = FALSE
  )
}

roc_plot(company_pred, test_set[, target], "orange")
roc_plot(title_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("orange", "blue"),
  c("Job Title", "Company"),
  lwd = 2
)
```

## Model Evaluation

```{r}
lime_plot <- function(model, features, pred) {
  test_cases <- c()

  # True Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # True Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  example <- test_set[test_cases, features]
  explainer <- lime(
    training_set[features],
    model = model,
    bin_continuous = TRUE,
    n_bins = 10
  )

  explanation <- explain(
    example,
    explainer,
    n_labels = 1,
    n_features = length(features)
  )
  plot_features(explanation)
}
```

```{r}
evaluate_model <- function(model, features, name) {
  pred <- predict(
    model,
    test_set[features],
    "prob"
  )[2]
  pred <- unlist(pred)

  auc <- calc_auc(pred, test_set[target])
  log_likelihood <- calc_log_likelihood(pred, test_set[, target])
  print(paste("AUC:", auc))
  print(paste("Log Likelihood:", log_likelihood))

  model_evaluations[nrow(model_evaluations) + 1, ] <- c(
    name,
    "multivariate",
    auc,
    log_likelihood
  )
  assign("model_evaluations", model_evaluations, envir = .GlobalEnv)

  plot(double_density_plot(
    pred,
    test_set[target],
    name,
    "Density"
  ))
  pred
}

make_pred <- function(model, features) {
  pred <- predict(
    model,
    test_set[features],
  )
  as.logical(pred)
}
```

## Naive Bayes

```{r, warning=FALSE}
naive_bayes <- caret::train(
  x = training_set[selected_features],
  y = as.factor(training_set[, target]),
  method = "nb"
)
```

```{r, warning=FALSE, fig.width=5, fig.height=3}
naive_bayes_pred <- evaluate_model(
  naive_bayes,
  selected_features,
  "Naive Bayes"
)
```

```{r, warning=FALSE, fig.width=12, fig.height=8}
pred <- make_pred(naive_bayes, selected_features)
kable(table(test_set[, target], pred))
lime_plot(naive_bayes, selected_features, pred)
```

## Logistic Regression

Logistic regression can be used to classify a variable dependent on one or more independent features. It will find the best fitting model to describe the relationship between the dependent and the independent variables. As it is a binary classification task `binomial` distribution is used.

```{r, warning=FALSE}
probability_columns <- paste(selected_features, "pred", sep = "_")
logistic_regression <- caret::train(
  x = training_set[probability_columns],
  y = as.factor(training_set[, target]),
  method = "glm",
  family = binomial(link = "logit")
)
```

```{r, warning=FALSE, fig.width=5, fig.height=3}
logistic_regression_pred <- evaluate_model(
  logistic_regression,
  probability_columns,
  "Logistic Regression"
)
```

```{r, warning=FALSE, fig.width=12, fig.height=8}
pred <- make_pred(logistic_regression, probability_columns)
kable(table(test_set[, target], pred))
lime_plot(logistic_regression, probability_columns, pred)
```

```{r}
summary(logistic_regression)
```

# Clustering 

The goal of clustering is to discover similarities among subsets of the data. 
Set the dataframe to be used in clustering. We are investigating the clustering of numerical and integer variables, that is the variables reporting on the job pay and ratings.

```{r}

clust_df = df
head(clust_df)

num_var <- colnames(df[
  sapply(df, class) %in% c("numeric", "integer")
])

```

Next, standardize the variables to ensure the disparity in scale does not adversely affect clustering 

```{r}
scaled_df <- scale(clust_df[,num_var])
head(scaled_df)
```

## Hierarchial Clustering

We are using hierarchical clustering to explore the possible splits in our data. 

Using the distance metric 'euclidean', output a dendrogram - a tree that represents nested clusters
```{r}

# Fristly, calculate the distance between each observation
dist_matrix <-  dist(scaled_df, method = "euclidean")

# Next, perform hierarchial cluster on the distance matrix trying different linkage methods 

n_clust = 4

# Single Linkage 
clust1 <-  hclust(dist_matrix, method <- "single")

plot(clust1, labels=FALSE, main="Cluster Dendrogram for Single Linkage")
# rect.hclust(clust1, k=n_clust)

# Calculate the clusters
clust2 <-  hclust(dist_matrix, method <- "complete")

plot(clust2, labels=FALSE, main="Cluster Dendrogram for Complete Linkage") 
rect.hclust(clust2, k=n_clust)

# Calculate the clusters
clust3 <-  hclust(dist_matrix, method <- "average")

plot(clust3, labels=FALSE, main="Cluster Dendrogram for Average Linkage")
rect.hclust(clust3, k=n_clust)

# Calculate the clusters
clust4 <-  hclust(dist_matrix, method <- "ward.D")

# Plot the dendrogram
plot(clust4,labels=FALSE, main="Cluster Dendrogram for Ward.D Linkage")
rect.hclust(clust4, k=n_clust)

# Calculate the clusters
clust4 <-  hclust(dist_matrix, method <- "ward.D2")

# Plot the dendrogram
plot(clust4,labels=FALSE, main="Cluster Dendrogram for Ward.D2 Linkage")
rect.hclust(clust4, k=n_clust)


```


Cutree takes in a clustering model and returns a vector of cluster group assignments for each row. We specify the number of clusters by using k = 3
```{r}
# Cut the dendrogram to form 4 clusters
clusters <- cutree(clust4, n_clust)

# Function to print the clusters 
print_clusters <- function(df, clusters, cols_to_print) {
  n_clusters <- max(clusters)
  for (i in 1:n_clusters) {
    print(paste("cluster", i))
    print(df[clusters == i, cols_to_print])
  }
}

cols_to_print <- c('Company.Revenue', num_var)
print_clusters(clust_df, clusters, cols_to_print)
clust_df$cluster <- as.factor(clusters)
```

```{r}
#gives clusters at different cluster levels in a table 
dend_classes9 = cutree(clust4, k = 1:9) # cuts the tree at various levels to get maximum of k clusters

#number of memeber
dend_classes9.fp = as.data.frame( dend_classes9 ) %>%
  pivot_longer(cols = 1:9,
               names_to = "max_clust",
               values_to = "cluster"
               )

conf = xtabs( ~ cluster + max_clust, dend_classes9.fp )
conf

The top 2 highest performing single model variables `Job.Title` and `Company` are also indicated as the 2 most significant variables shown under the *Coefficients* part of the summary as their p-value are much smaller than others.
 
### Principal Component Analysis 

## Comparison

```{r, fig.width=5, fig.height=5}
kable(model_evaluations)

roc_plot(naive_bayes_pred, test_set[, target], "orange")
roc_plot(logistic_regression_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("orange", "blue"),
  c("Naive Bayes", "Logistic Regression"),
  lwd = 2
)
```

```{r}
# Focus on the first two principal components
nComp <- 2 
project2D <- as.data.frame(predict(PCA, newdata=scaled_df)[,1:nComp])
project2D

hclust.project2D <- cbind(project2D, cluster=as.factor(clusters), sector=df$Company.Sector)
head(hclust.project2D)
```

Plot convex hull
```{r}
find_convex_hull <- function(proj2Ddf, clusters) {
  do.call(rbind,
    lapply(unique(clusters),
      FUN = function(c) {
        f <- subset(proj2Ddf, cluster==c);
        f[chull(f),]
      }
      )
    )
  }
hclust.hull <- find_convex_hull(hclust.project2D, clusters)
```


```{r}
ggplot(hclust.project2D, aes(x=PC1, y=PC2)) +
geom_point(aes(shape=cluster, color=cluster)) +
# geom_text(aes(label=sector, color=cluster), hjust=0, vjust=1, size=3) +
geom_polygon(data=hclust.hull, aes(group=cluster, fill=as.factor(cluster)),
alpha=0.4, linetype=0) + theme(text=element_text(size=20))
```


### FINDING THE NUMBER OF CLUSTERS 

The total within sum of squares (WSS) for different k values can help us to find the optimal number of clusters. The elbow of the curve indicates where the rate at which WSS slows down.

Alternatively, the Calinski-Harabasz index (CH index) is another measure of cluster goodness.

A good cluster has a small WSS (all the clusters are tight around their centres) and a large BSS (large seperation between clusters)

CH = B/W

Maximise CH index

```{r}
# Function to return the squared Euclidean distance of two given points x and y
sqr_euDist <- function(x, y) {
sum((x - y)^2)
}
# Function to calculate WSS of a cluster, represented as a n-by-d matrix
# (where n and d are the numbers of rows and columns of the matrix)
# which contains only points of the cluster.
wss <- function(clustermat) {
c0 <- colMeans(clustermat)
sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}
# Function to calculate the total WSS. Argument `scaled_df`: data frame
# with normalised numerical columns. Argument `labels`: vector containing
# the cluster ID (starting at 1) for each row of the data frame.
wss_total <- function(scaled_df, labels) {
wss.sum <- 0
k <- length(unique(labels))
for (i in 1:k)
wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
wss.sum
}

# Function to calculate total sum of squared (TSS) distance of data
# points about the (global) mean. This is the same as WSS when the
# number of clusters (k) is 1.
tss <- function(scaled_df) {
wss(scaled_df)
}

# Function to return the CH indices computed using hierarchical
# clustering (function `hclust`) or k-means clustering (`kmeans`)
# for a vector of k values ranging from 1 to kmax.
CH_index <- function(scaled_df, kmax, method="kmeans") {
if (!(method %in% c("kmeans", "hclust")))
stop("method must be one of c('kmeans', 'hclust')")
npts <- nrow(scaled_df)
wss.value <- numeric(kmax) # create a vector of numeric type
# wss.value[1] stores the WSS value for k=1 (when all the
# data points form 1 large cluster).
wss.value[1] <- wss(scaled_df)
if (method == "kmeans") {
# kmeans
for (k in 2:kmax) {
clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
wss.value[k] <- clustering$tot.withinss
}
} else {
# hclust
d <- dist(scaled_df, method="euclidean")
pfit <- hclust(d, method="ward.D2")
for (k in 2:kmax) {
labels <- cutree(pfit, k=k)
wss.value[k] <- wss_total(scaled_df, labels)
}
}
bss.value <- tss(scaled_df) - wss.value # this is a vector
B <- bss.value / (0:(kmax-1)) # also a vector
W <- wss.value / (npts - 1:kmax) # also a vector
data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
```

To plot the CH index vs k

```{r}
# calculate the CH criterion
crit.df <- CH_index(scaled_df, 10, method="hclust")
fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
theme(text=element_text(size=20))

grid.arrange(fig1, fig2, nrow=1)
```
The CH index is maximised at 2 clusters, whereas the wss does not appears to a clear elbow. 
Looking at the figure we can see that the CH criterion is maximized at k=2

### Assess whether the cluster represents true structure 

To see if the cluster holds up under plausible varations in the dataset use clusterboot(). Clusterboot() using bootstrap resampling to evaluate how stable a given cluster is

```{r}
library(fpc)

n_clust = 4
cboot.hclust <-  clusterboot(scaled_df, clustermethod = hclustCBI, method="ward.D2", k=n_clust)
```


```{r}
# summary(cboot.hclust$result)
clusters.cboot <- cboot.hclust$result$partition
# print_clusters(df, clusters.cboot, "Company.Industry")
```


Count how many times each cluster was dissolved 

```{r}
# cboot.hclust$bootbrd = number of times a cluster is desolved.
(values <- 1 - cboot.hclust$bootbrd/100) # large values here => highly stable
## [1] 0.75 0.85 0.51 0.90 0.67
cat("So clusters", order(values)[1], order(values)[3], "and", order(values)[4], "are highly stable")
## So clusters 4 and 2 are highly stable
```


#KMEANS

```{r}
nclust <- 3
# run kmeans with 5 clusters, 100 random starts, and 100
# maximum iterations per run.
km_clust <- kmeans(scaled_df, nclust, nstart=100, iter.max=100)
km_clust$centers
```

```{r}
kmClustering.ch <- kmeansruns(scaled_df, krange=1:10, criterion="ch")
kmClustering.ch$bestk
## [1] 2
kmClustering.asw <- kmeansruns(scaled_df, krange=1:10, criterion="asw")
kmClustering.asw$bestk
## [1] 3
# Compare the CH values for kmeans() and hclust().
print("CH index from kmeans for k=1 to 10:")
## [1] "CH index from kmeans for k=1 to 10:"
print(kmClustering.ch$crit)
## [1] 0.000000 14.094814 11.417985 10.418801 10.011797 9.964967 9.861682
## [8] 9.412089 9.166676 9.075569
print("CH index from hclust for k=1 to 10:")
## [1] "CH index from hclust for k=1 to 10:"
hclusting <- CH_index(scaled_df, 10, method="hclust")
print(hclusting$CH_index)
## [1] NaN 12.215107 10.359587 9.847229 10.011797 9.964967 9.552301
## [8] 9.139594 8.873366 8.751161
```

```{r}
kmCritframe <- data.frame(k=1:10, ch=kmClustering.ch$crit,
asw=kmClustering.asw$crit)
fig1 <- ggplot(kmCritframe, aes(x=k, y=ch)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(kmCritframe, aes(x=k, y=asw)) +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="ASW") + theme(text=element_text(size=20))
grid.arrange(fig1, fig2, nrow=1)
```

```{r}
fig <- c()
kvalues <- seq(2,5)
for (k in kvalues) {
clusters <- kmeans(scaled_df, k, nstart=100, iter.max=100)$cluster
kmclust.project2D <- cbind(project2D, cluster=as.factor(clusters),
state=df$State)
kmclust.hull <- find_convex_hull(kmclust.project2D, clusters)
assign(paste0("fig", k),
ggplot(kmclust.project2D, aes(x=PC1, y=PC2)) +
geom_point(aes(shape=cluster, color=cluster, alpha=0.2)) +
geom_polygon(data=kmclust.hull, aes(group=cluster, fill=cluster),
alpha=0.4, linetype=0) +
labs(title = sprintf("k = %d", k)) +
theme(legend.position="none", text=element_text(size=20))
)
}
grid.arrange(fig2, fig3, fig4, fig5, nrow=2)
```

### Analysing the clusters for patterns 

We will first look into the locations of the jobs from each cluster
```{r}


ggplot(clust_df, aes(cluster, fill=State)) + 
  geom_bar(position="fill")
# 
# clust_df$cluster
# 
# ggplot(df) +
#   geom_polygon(data=map_data("Australia"), aes(x = long, y = lat, group = group), fill="#F0F0F0", colour = "lightgray")+
#   geom_point(aes(x=longitude, y=latitude, color=cluster), alpha=0.5) +
#   coord_quickmap() +
#   cits4009_map_theme

```
As seen in the filled bar chart above, there is no distinct pattern to be found between the clusters and job location. 

Next we can consider the pay, and see if the clusters form around this. 

```{r}
ggplot(clust_df, aes(Estimate.Base.Salary, fill=cluster)) + 
  geom_histogram()
```
The histogram of estimate base salary, grouped by cluster shows evidence of patterns. 

```{r}
ggplot(clust_df, aes(Company.Rating, Estimate.Base.Salary, colour = factor( cluster ) ) ) + geom_point()
```
```{r}
aggregate(clust_df[,num_var], by=list(cluster=clust_df$cluster), mean)
```

