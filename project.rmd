---
title: "CITS4009 Computational Data Analysis"

graphics: yes
author: <i>Kaiqi LIANG (23344153)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Introduction

The dataset for this project is Australia's data science job listings for August 2022 which can be downloaded on [Kaggle](https://www.kaggle.com/datasets/nadzmiagthomas/australia-data-science-jobs). It was scrapped from [Glassdoor](https://www.glassdoor.com.au) which is a website where current and former employees anonymously review companies.

# Setup

Clean workspace and set random seed.

```{r}
rm(list = ls())
set.seed(1)
```

Code to allow truncating text output especially for `str()` and `head()` functions.

```{r}
# Save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# Set a new output hook to truncate text output
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x <- xfun::split_lines(x)
    if (length(x) > n) {
      # Truncate the output
      x <- c(head(x, n), "....\n")
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})
```

Load libraries and data.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(WVPlots)
library(ozmaps)
library(sf)
library(dplyr)
library(gridExtra)
library(knitr)
library(ROCR)
library(ROCit)
library(caret)
library(lime)
library(grDevices)
library(tidyverse)
library(fpc)
library(klaR)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)

# Replace empty strings with NA
df[df == ""] <- NA
```

Set up a custom theme so all the plots can be standardised as well as having the ability to customise to their needs.

```{r}
custom_theme <- function(
  title_y_r_margin = 10,
  title_x_t_margin = 10,
  title_b_margin = 20,
  title_size = 20,
  label_size = 16,
  font_size = 14
) {
  theme(
    axis.title.x = element_text(margin = margin(t = title_x_t_margin)),
    axis.title.y = element_text(margin = margin(r = title_y_r_margin)),
    axis.title = element_text(size = label_size),
    plot.title = element_text(
      size = title_size,
      margin = margin(b = title_b_margin)
    ),
    text = element_text(size = font_size)
  )
}
```

# Data Cleaning and Transformation

## Data Exploratory, Cleaning 

Use `str()` to have a glance at the data.

```{r, out.lines=30}
str(df)
```

There are 2088 observations of 53 variables.

## Renaming Variables

There are a couple typos in the variable names let's fix that first, and some of them are very verbose which we can simplify.

```{r}
df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)
```

## Dropping Columns

The variable `Country` should be all `Australia` because this is an Australia's data science jobs dataset, all the job listings should be in Australia, otherwise it is not valid.

```{r}
any(df$Country != "Australia")
```

Confirmed that there is no invalid data for the `Country` variable, then it is essentially useless as all the observations are the same, so we should be able to drop it, along with some other columns that are just long chunk of text, `Job Descriptions` and `Url` in particular which are also not that useful to us.

```{r}
df <- df[!names(df) %in% c("Country", "Job.Descriptions", "Url")]
```

## Type Conversion

All the variables that end with `_yn` contain only `1`s and `0`s which should be converted to `logical`.

```{r}
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})
```

Then inspect the data again using `head()` to look at the first 3 observations.

```{r, out.lines=28}
head(df, 3)
```

Looking much cleaner now.

## Missing Values

Count the missing values in each variable.

```{r}
count_missing <- function() {
  na_counts <- sapply(df, function(col) length(which(is.na(col))))
  na_counts[which(na_counts > 0)]
}
count_missing()
```

The 3 jobs that do not even have a `Title` should be dropped.

```{r}
df <- df[!is.na(df$Job.Title), ]
count_missing()
```

The variable `Company Founded` is the year when the company is founded in and `Company CEO Approval` is the percentage of employees who approve their CEO. They both have around 40% of missing values, the first one is probably because the companies did not enter the year founded and the second one might be because the employees are scared to say about their CEO. Due to these reasons they do not have enough useful information to be kept.

```{r}
df <- df[!names(df) %in% c("Company.Founded", "Company.CEO.Approval")]
count_missing()
```

All 3 variables `Company Size`, `Company Type` and `Company Revenue` are missing 180 observations, let's see if they are the same ones.

```{r}
summary(cbind(
  Company.Size = which(is.na(df$Company.Size)),
  Company.Type = which(is.na(df$Company.Type)),
  Company.Revenue = which(is.na(df$Company.Revenue))
))
```

The missing values in all 3 columns appear to be in the exact same locations. As it is almost 10% of the total number of observations, let's see if they are also missing the rating data.

```{r}
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
any(!is.na(df[is.na(df$Company.Size), rating_columns]))
```

Looks like all the 180 jobs are missing the rating data as well, we can just remove them all.

```{r}
df <- df[!is.na(df$Company.Size), ]
count_missing()
```

Now there are still 130 missing values for `Company Rating` and `Number of Rater`, again check whether they're from the same observations.

```{r}
any(is.na(df$Company.Rating) != is.na(df$Number.of.Rater))
```

Not a single `NA` in `Company Rating` is different from the ones in `Company Number of Rater` which means they are indeed from the same observations. This makes sense because if the number of rater doesn't exist then there shouldn't be any ratings either, so the missing values in `Number of Rater` can be replaced with 0 indicating no one rated and the rating is an approximation based off the mean rating.

```{r}
df$Number.of.Rater[is.na(df$Number.of.Rater)] <- 0
```

Have a peak at the summary of the rating columns.
```{r}
summary(df[rating_columns])
```

Since the `Company Rating` is just a number out of 5 we can replace them with the mean value, similarly for `Career Opportunities`, `Compensation and Benefits`, `Culture and Values`, `Senior Management` and `Work Life Balance`. With regards to `Friend Recommendation`, it is just another rating but out of 100, we can do the same for all of them.

```{r}
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- mean(!na)
  column
})
count_missing()
```

The 2 variables left both have 386 missing values, check one last time if they are from the same observations.

```{r}
any(is.na(df$Company.Sector) != is.na(df$Company.Industry))
```

Same result as before, and since they are categorical, their missing values can just be replaced with a new category `Unknown`.

```{r}
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})
count_missing()
```

Finally there are no more missing values.

# Visualisation

## Job Locations

```{r}
df_locations <- as.data.frame(table(df$Job.Location))
colnames(df_locations) <- c("Location", "Number.of.Jobs")

ggplot(
  df_locations[df_locations$Number.of.Jobs > 10, ],
  aes(
    x = reorder(Location, Number.of.Jobs),
    y = Number.of.Jobs
  ),
) +
  geom_bar(
    stat = "identity",
    width = 0.6,
    fill = "darkcyan"
  ) +
  geom_text(
    aes(label = Number.of.Jobs),
    hjust = 0
  ) +
  labs(
    x = "Job Location",
    title = "Locations with the highest number of jobs"
  ) +
  annotate("text", x = 12, y = 200, label = "This is invalid") +
  annotate("segment", x = 12, y = 140, xend = 12, yend = 80, arrow = arrow(
    type = "closed", length = unit(0.02, "npc")
  )) +
  coord_flip() +
  custom_theme(
    title_b_margin = 10,
    title_size = 14,
    label_size = 12,
    font_size = 11
  )
```

The bar chart shows the locations in decreasing order in the number of job openings where there are at least 10.

The 6th location `Australia` should be changed to `Unknown` as all the other locations are cities other than countries. The reason for this category to appear could either be there are multiple locations in Australia or not sure the exact location.

```{r}
df$Job.Location[df$Job.Location == "Australia"] <- "Unknown"
```

Let's look at the distribution in terms of states.

```{r, out.width="50%"}
ClevelandDotPlot(
  df,
  "State",
  sort = 1,
  title = "Jobs Distribution by State"
) +
  coord_flip() +
  custom_theme(title_y_r_margin = 20, title_x_t_margin = 5)
```

Just looking at the first 5 locations they are all capital cities of their state which makes the state ranking the same. We can plot the Australia states map to see the jobs distribution more clearly.

```{r, out.width="50%"}
sf_oz <- ozmap_data("states")

jobs <- as.data.frame(table(df$State))
colnames(jobs) <- c("NAME", "Jobs")

ggplot(merge(sf_oz, jobs, all.x = TRUE)) +
  geom_sf(aes(fill = Jobs)) +
  labs(
    x = "Longitude",
    y = "Latitude"
  ) +
  scale_fill_gradient(low = "purple", high = "lightpink") +
  custom_theme()
```

## Company

```{r, out.width="65%"}
df_companies <- as.data.frame(table(df$Company))
colnames(df_companies) <- c("Company", "Number.of.Jobs")

df_industries <- df[!duplicated(df$Company), c("Company", "Company.Industry")]
df_merge <- merge(df_companies, df_industries)

ggplot(
  head(df_merge[order(-df_merge$Number.of.Jobs), ], 10),
  aes(
    x = reorder(Company, -Number.of.Jobs),
    y = Number.of.Jobs,
    fill = Company.Industry
  )
) +
  geom_bar(
    stat = "identity",
  ) +
  geom_text(
    aes(label = Number.of.Jobs),
    vjust = 1.5,
    colour = "white"
  ) +
  labs(
    x = "Company",
    title = "Top 10 companies in number of job offerings"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  custom_theme(
    title_b_margin = 10,
    title_size = 14,
    label_size = 12,
    font_size = 10
  )
```

The company `Deloitte` which is in the `Accounting & Tax` industry has the most job openings. The number almost doubled the second most company `CSIRO` which is a `National Service & Agencies` and the rest are quite close, this is probably because `Accounting & Tax` requires more data scientists as they deal with numbers the most.

Convert `Company Size` and `Company Revenue` to `factor` so they can be manually ordered based on the categories.

```{r, out.width="100%"}
df$Company.Size <- factor(
  df$Company.Size,
  levels = c(
    "Unknown",
    "1 to 50 Employees",
    "51 to 200 Employees",
    "201 to 500 Employees",
    "501 to 1000 Employees",
    "1001 to 5000 Employees",
    "5001 to 10000 Employees",
    "10000+ Employees"
  )
)
df$Company.Revenue <- factor(
  df$Company.Revenue,
  levels = c(
    "Unknown / Non-Applicable",
    "Less than $1 million (USD)",
    "$1 to $5 million (USD)",
    "$5 to $10 million (USD)",
    "$10 to $25 million (USD)",
    "$25 to $50 million (USD)",
    "$50 to $100 million (USD)",
    "$100 to $500 million (USD)",
    "$500 million to $1 billion (USD)",
    "$1 to $2 billion (USD)",
    "$2 to $5 billion (USD)",
    "$5 to $10 billion (USD)",
    "$10+ billion (USD)"
  )
)
summary(df[c("Company.Size", "Company.Revenue")])
```

Let's see if there is a relationship between `Company Size` and `Company Revenue`, plot a stacked bar chart excluding `Unknown` or `NA` options.

```{r}
ggplot(
  df[df$Company.Revenue != "Unknown / Non-Applicable" &
    !duplicated(df$Company), ]
) +
  geom_bar(
    aes(x = Company.Revenue, fill = Company.Size),
    alpha = 0.5
  ) +
  labs(y = "Number of Companies") +
  theme(
    axis.text.x = element_text(angle = 75, hjust = 1.05),
    axis.title = element_text(size = 16)
  ) +
  custom_theme(title_x_t_margin = 15, font_size = 10, label_size = 12)
```

Looks like in general the bigger the company is the higher their revenue is. This can be seen through a couple observations, small companies with fewer than 50 employees only have a revenue less than 25 million, medium sized companies with 501 to 1000 employees have a revenue between 50 million to 2 billion, and most of the large companies with more than 10000 employees have a revenue of over 10 billion.

Another observation that can be made is there are more companies with higher revenue, which is because the bigger the company is the more they want to grow therefore more likely to have open positions.

```{r, fig.width=8, fig.height=5.5}
ggplot(df[df$Company.Sector != "Unknown", ]) +
  geom_count(aes(x = Company.Sector, y = Company.Type, colour = ..n..)) +
  theme(axis.text.x = element_text(angle = 75, hjust = 1)) +
  scale_color_gradient(low = "brown", high = "magenta")
```

Plotting `Company Type` against `Company Sector`. Some make perfect sense for example all `College / University` are in `Education` and all `Hospital` are in `Healthcare`. Some are more interesting to look at like `Public` and `Private` as we can see they both have a big focus on `Finance` and `Information Technology` while `Public` companies also have an emphasis on `Pharmaceutical & Biotechnology` and `Private` companies are in `Human Resources & Staffing` sector where `Public` companies are not.

## Salary

```{r, out.width="50%"}
salary_columns <- c("High.Estimate", "Estimate.Base.Salary", "Low.Estimate")
df_salary <- stack(df[salary_columns])
colnames(df_salary) <- c("amount", "salary")

ggplot(df_salary) +
  geom_boxplot(
    aes(x = salary, y = amount, fill = salary),
    outlier.colour = "red"
  ) +
  scale_y_continuous(labels = scales::dollar_format()) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  custom_theme(title_x_t_margin = 15)
```

In job descriptions the salary package is often provided as a range rather than a fixed number as there is room for negotiation, the lower end of the range is `Low Estimate` and the higher end is `High Estimate`, `Estimate Base Salary` is the average of the two.

As we can see from the box plot there are many outliers with really high starting salary and no outliers on the minimum wage. Since all 3 variables have similar distribution we'll just use the `Estimate Base Salary` for base salaries from now on as it is more representative.

Let's find out who the outliers are that offer such high base salary.

```{r, message=FALSE, out.width="65%"}
df_salary <- df[
  !duplicated(df$Estimate.Base.Salary),
  c(
    "Company",
    "Estimate.Base.Salary",
    "Company.Revenue"
  )
]

df_salary[order(-df_salary$Estimate.Base.Salary), ] %>%
  head(10) %>%
  ggplot() +
    geom_point(aes(
      x = Company.Revenue,
      y = Estimate.Base.Salary,
      colour = Company
    )) +
    labs(title = "Top 10 Highest Paying Jobs") +
    scale_y_continuous(labels = scales::dollar_format()) +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    custom_theme(
      title_b_margin = 10,
      title_size = 14,
      label_size = 12,
      font_size = 10
    )
```

Looks like a couple multi billion dollar companies are, especially `Indeed` who offers some of the highest income jobs, which makes sense as they have the revenue to do so. However majority of the jobs still have a starting salary of around $100,000 as shown by the histogram and density plot below.

```{r, out.width="50%"}
ggplot(df, aes(x = Estimate.Base.Salary)) +
  geom_histogram(
    aes(y = ..density..),
    binwidth = 8000,
    alpha = 0.8,
    colour = "skyblue",
    fill = "lightblue"
  ) +
  geom_density(colour = "darkred") +
  scale_x_continuous(labels = scales::dollar_format()) +
  theme(axis.text.x = element_text(hjust = 1, vjust = -0.5)) +
  custom_theme(title_y_r_margin = 15, title_x_t_margin = 15)
```

## Rating

```{r, out.width="50%", message=FALSE}
ggplot(df, aes(x = Company.Rating, y = Estimate.Base.Salary)) +
  geom_point(colour = "darkgreen", shape = 23) +
  geom_smooth() +
  scale_y_continuous(labels = scales::dollar_format()) +
  custom_theme()
```

There is a small trend as the higher rating the company receives the higher base salary it pays, but the fitted curve is strongly affected by outliers as there are a lot of 5 star ratings that pay very low salary. This might be due to a portion of people who care more about other things like company culture than money.

```{r, out.width="100%"}
df_ratings <- df[rating_columns]

# Replace dots with newlines so that separate words are on
# different lines to fit in the scatter matrix
colnames(df_ratings) <- gsub("\\.", "\n", rating_columns)

pairs(df_ratings)
```

As suspected all the other criteria seem to be linearly correlated to the `Company Rating`, so the overall rating is more closely related to all the criteria than salary, ie. high salary does not necessarily give the company a good rating.

## Skill

```{r, out.width="50%", fig.height=3}
javascript_r <- ggplot(
  count(df, javascript_yn, r_yn),
  aes(x = javascript_yn, y = r_yn),
) +
  geom_tile(aes(fill = n)) +
  coord_equal() +
  labs(y = "R", x = "JavaScript") +
  theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) +
  custom_theme(font_size = 10, label_size = 12)

python_r <- ggplot(
  count(df, python_yn, r_yn),
  aes(x = python_yn, y = r_yn),
) +
  geom_tile(aes(fill = n)) +
  coord_equal() +
  labs(y = "", x = "Python") +
  custom_theme(font_size = 10, label_size = 12)

grid.arrange(javascript_r, python_r, ncol = 2)

# Count the number of jobs that require JavaScript which also require R
sum(which(df$javascript_yn) %in% which(df$r_yn))
```

Not a single job requires both `JavaScript` and `R` but there are a lot of overlaps between `Python` and `R`. There are more jobs that prefer `R` over `JavaScript` but `Python` over `R`.

# Classification

## Target Variable

Sometimes job postings do not include the salary. A classification model that predicts the salary range of future job postings, based on past job postings with salary information, would be incredibly useful for job hunters who prioritise salary when applying for jobs. As such, the response variable we have chosen is the salary level of the job posting.

The original dataset included three numeric salary variables: `High.Estimate`, `Estimate.Base.Salary`, and `Low.Estimate`. In job descriptions the salary package is often provided as a range rather than a fixed number as there is room for negotiation, the lower end of the range is `Low.Estimate` and the higher end is `High.Estimate`, `Estimate.Base.Salary` is the average of the two. Hence focusing on the `Estimate.Base.Salary` this can be a binary classification problem of whether the salary is high. The cutoff between these two classes is the median of the `Estimate.Base.Salary`, $95,000, so one class has a salary between $0 - $95,000 and is assigned a label of `FALSE`, the other is a salary above $95,000 and is assigned a label of `TRUE`. Since the cutoff is based on the median, the dataset is almost balanced.

```{r}
target <- "High.Salary"
df[, target] <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)

paste(
  "There are",
  nrow(df[df[, target], ]),
  "high salary observations. There are",
  nrow(df[!df[, target], ]),
  "low salary observations."
)
```

## Feature Variables

Identify the categorical and numerical feature variables.

```{r}
features <- setdiff(colnames(df), target)

# Convert character to factor
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)

categorical_variables <- features[
  sapply(df[, features], class) %in% c("factor", "logical")
]
numerical_variables <- features[
  sapply(df[, features], class) %in% "numeric"
]

paste(
  "There are",
  length(categorical_variables),
  "categorical features,",
  length(numerical_variables),
  "numerical features",
  "and 1 target column."
)
```

Convert numerical variables to categorical so that they can be used to create single variable models, but still keep the original numerical variables for clustering.

```{r}
numerical_to_categorical <- c()
for (variable in numerical_variables) {
  categorical_variable <- paste(variable, "categorical", sep = "_")
  numerical_to_categorical <- c(numerical_to_categorical, categorical_variable)
  if (variable == "Friend.Recommendation") { # A rating out of 100
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 100, 10)
    )
  } else { # Ratings out of 5
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 5, 0.5)
    )
  }
}
```

## Test and Training Sets

Perform a 80/20 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
training_set <- df[split < 0.8, ]
test_set <- df[split >= 0.8, ]

paste(
  "The training and test set has",
  nrow(training_set),
  "and",
  nrow(test_set),
  "observations respectively."
)
```

## Null model

The null model will always return the the majority category. As mentioned earlier, the dataset is almost balanced so the model will have a 50% chance of predicting `High.Salary (TRUE)`, resulting in an Area Under the Curve (AUC) of 0.5.

```{r}
calc_auc <- function(pred, ground_truth) {
  round(as.numeric(
    performance(prediction(pred, ground_truth), "auc")@y.values
  ), 4)
}

calc_log_likelihood <- function(pred, ground_truth) {
  pred <- pred[pred > 0 & pred < 1]
  round(sum(ifelse(ground_truth, log(pred), log(1 - pred))))
}

null_model <- sum(training_set[target]) / nrow(training_set)
```

Create a data frame to store the AUC and Log Likelihood of different models.

```{r}
# Function to calculate the AUC and Log Likelihood then store their values
# to the global data frame which contains the same values for other models
calc_auc_log_likelihood <- function(pred, name, type) {
  auc <- calc_auc(pred, test_set[target])
  log_likelihood <- calc_log_likelihood(pred, test_set[, target])
  print(paste("AUC:", auc))
  print(paste("Log Likelihood:", log_likelihood))

  model_evaluations[nrow(model_evaluations) + 1, ] <- c(
    name,
    type,
    auc,
    log_likelihood
  )
  assign("model_evaluations", model_evaluations, envir = .GlobalEnv)
}

model_evaluations <- data.frame(
  Model.Name = "Null Model",
  Model.Type = "univariate",
  AUC = calc_auc(rep(null_model, nrow(test_set)), test_set[target]),
  Log.Likelihood = calc_log_likelihood(null_model, test_set[, target])
)
kable(model_evaluations)
```

## Single Variable Model

Single variable model based on categorical variables.

```{r}
single_variable_prediction <- function(pred_col, output_col, test_col) {
  t <- table(pred_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}

cross_validation_100_fold <- function(variable) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(training_set), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      training_set[split, variable],
      training_set[split, target],
      training_set[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, training_set[!split, target])
  }
  mean(aucs)
}
```

Find the average AUC for each variable over 100 fold cross validation and save the predicted probabilities back to the data frame so that they can be used in Logistic Regression.

```{r}
single_variable_models <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(single_variable_models) <- c("Variable", "AUC")
for (variable in c(numerical_to_categorical, categorical_variables)) {
  auc <- cross_validation_100_fold(variable)
  single_variable_models[nrow(single_variable_models) + 1, ] <-
    c(variable, auc)

  training_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      training_set[, variable]
    )

  test_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      test_set[, variable]
    )
}
```

Select the variables with an average AUC higher than 0.6 to be the features used in model trainings later on.

```{r}
selected_models <-
  single_variable_models[
    single_variable_models$AUC > 0.6,
  ]
selected_features <- selected_models$Variable

selected_models <-
  selected_models[
    order(selected_models$AUC, decreasing = TRUE),
  ]
row.names(selected_models) <- NULL

kable(selected_models)
```

Pick the top 2 single variable models based on their average AUC which are `Company` and `Job.Title` having a unusually high value of almost 0.9 and 0.8. However this makes perfect sense as within the same company most data science jobs will very likely to have the same salary, and similarly the exact same job title usually would not have a big difference in their salaries.

```{r}
company_pred <- single_variable_prediction(
  training_set$Company,
  training_set[, target],
  test_set$Company
)
job_title_pred <- single_variable_prediction(
  training_set$Job.Title,
  training_set[, target],
  test_set$Job.Title
)

# Calculate their AUC and Log Likelihood using the test set.
calc_auc_log_likelihood(company_pred, "Company", "univariate")
calc_auc_log_likelihood(job_title_pred, "Job Title", "univariate")
```

The metrics measured on the test set are higher than on the validation set because the latter was the average across 100 folds.

Plot their predicted probabilities next to each other.

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Salary = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Salary)) +
    labs(x = paste("Predicated Probability of", x), y = y)
}

grid.arrange(
  double_density_plot(
    company_pred,
    test_set[target],
    "Company",
    "Density"
  ),
  double_density_plot(
    job_title_pred,
    test_set[target],
    "Job Title",
    ""
  ),
  ncol = 2
)
```

Compare the ROC curves by plotting them on top of each other.

```{r, fig.width=5, fig.height=5}
roc_plot <- function(pred_col, out_col, colour = "red", overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour, "black"),
    legend = FALSE,
    YIndex = FALSE
  )
}

roc_plot(company_pred, test_set[, target], "red")
roc_plot(job_title_pred, test_set[, target], "blue", TRUE)
legend(
  "bottomright",
  col = c("red", "blue"),
  c("Company", "Job Title"),
  lwd = 2
)
```

As shown above using the AUC metric, `Company` performs moderately better than `Job.Title` however using the Log Likelihood metric, `Job.Title` is better. This means `Company` has a higher performance averaged across all possible decision thresholds but `Job.Title` gives a higher certainty in its predictions as Log Likelihood measures how close the predicted probabilities are to the ground truth (0 or 1).

## Model Evaluation

Functions to call on a model to make predictions, calculate the AUC and Log Likelihood, evaluate which features play a key role in determining if a job posting offers high or low salary.

```{r}
# Function to print the confusion matrix as well as calculating
# the precision and recall
confusion_matrix_accuracy <- function(model, features) {
  pred <- as.logical(predict(
    model,
    test_set[features],
  ))

  confusion_matrix <- table(
    ifelse(test_set[, target], "High Salary", "Low Salary"),
    pred
  )[, 2:1]

  print(paste(
    "Precision:",
    format(confusion_matrix[1, 1] / sum(confusion_matrix[, 1]), digits = 3)
  ))

  print(paste(
    "Recall:",
    format(confusion_matrix[1, 1] / sum(confusion_matrix[1, ]), digits = 3)
  ))

  print(kable(confusion_matrix))
  pred
}

# Function to calculate the AUC and Log Likelihood as well as generating a
# double density plot
evaluate_model <- function(model, features, name) {
  pred <- predict(
    model,
    test_set[features],
    "prob"
  )[2]
  pred <- unlist(pred)

  calc_auc_log_likelihood(pred, name, "multivariate")

  plot(double_density_plot(
    pred,
    test_set[target],
    name,
    "Density"
  ))
  pred
}

# Function to plot the explanation of how the individual features
# support or contradict a prediction
lime_plot <- function(model, features, pred) {
  # Pick 4 examples for LIME to explain
  test_cases <- c()

  # True Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Negative
  for (i in seq(length(pred))) {
    if (test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Positive
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # True Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  example <- test_set[test_cases, features]
  explainer <- lime(
    training_set[features],
    model = model,
    bin_continuous = TRUE,
    n_bins = 10
  )

  explanation <- explain(
    example,
    explainer,
    n_labels = 1,
    n_features = length(features)
  )
  plot_features(explanation)
}
```

The LIME plot will explain 4 test cases, top left is a True Positive instance, top right is a False Negative instance, bottom left is a False Positive instance and bottom right is a True Negative instance.

## Naive Bayes

Naive Bayes classifier works well on categorical variables which is why it is chosen for this problem as there are many categorical variables with lots of levels.

```{r, warning=FALSE}
naive_bayes <- caret::train(
  x = training_set[selected_features],
  y = as.factor(training_set[, target]),
  method = "nb"
)
```

Naive Bayes has an AUC of 0.9667 and a Log Likelihood of -116 which outperforms the highest single variable model of `Company` with an AUC of 0.904 and a Log Likelihood of -401. This means Naive Bayes is doing a great job of combining the top performing variables to improve its predictions even further.

```{r, warning=FALSE, fig.width=5, fig.height=3}
naive_bayes_pred <- evaluate_model(
  naive_bayes,
  selected_features,
  "Naive Bayes"
)
```

The number of False Positive and False Negative cases are the same which results in the exact same precision and recall.

```{r, warning=FALSE}
pred <- confusion_matrix_accuracy(naive_bayes, selected_features)
```

Except for the False Negative case shown by the top right LIME plot, 2 of the No.1 determining features are `Company` which is the top performing single variable model, and the other is `Job.Title` which is the second best variable.

```{r, warning=FALSE, fig.width=12, fig.height=8}
lime_plot(naive_bayes, selected_features, pred)
```

## Logistic Regression

Logistic Regression can be used to classify a variable dependent on one or more independent features. It will find the best fitting model to describe the relationship between the dependent and the independent variables. As it is a binary classification task `binomial` distribution is used.

Due to every categorical variable has to be expanded to a set of indicator variables in Logistic Regression, it does not work well with large number of levels. Therefore instead of using the original data whose categorical variables contain many levels, the predicted probabilities from the single variable models will be used.

```{r, warning=FALSE}
probability_columns <- paste(selected_features, "pred", sep = "_")
logistic_regression <- caret::train(
  x = training_set[probability_columns],
  y = as.factor(training_set[, target]),
  method = "glm",
  family = binomial(link = "logit")
)
```

Logistic Regression has an AUC of 0.9672 and a Log Likelihood of -120 which is pretty much the same as Naive Bayes. However the difference between them is in the precision and recall.

```{r, warning=FALSE, fig.width=5, fig.height=3}
logistic_regression_pred <- evaluate_model(
  logistic_regression,
  probability_columns,
  "Logistic Regression"
)
```

Logistic Regression has a lower precision but a higher recall than Naive Bayes which means it is able to identify more of the high salary jobs than Naive Bayes while making more mistakes predicting low salary jobs as high salary.

```{r}
pred <- confusion_matrix_accuracy(logistic_regression, probability_columns)
```

Once again except for the False Negative case shown by the top right LIME plot, the predicted probabilities from the `Company` single varible model is the most supporting feature which is also the top performing variable.

```{r, warning=FALSE, fig.width=12, fig.height=8}
lime_plot(logistic_regression, probability_columns, pred)
```

The top 2 highest performing single model variables `Company` and `Job.Title` are also indicated as the 2 most significant variables shown under the *Coefficients* part of the summary as their p-value are much smaller than others.

```{r}
summary(logistic_regression)
```

## Comparison

There are a couple single variable models that perform even worse than the null model which has an AUC of 0.5 on this balanced dataset. However the majority of them perform quite well and only the AUCs above 0.6 will be used as the features for training the classification models. Therefore these low performing features will not affect the performance of the models.

```{r}
paste0(
  round(
    nrow(single_variable_models[single_variable_models$AUC > 0.5, ]) /
    nrow(single_variable_models) *
    100
  ),
  "% of the single variable models perform better than the null model."
)

single_variable_models <-
  single_variable_models[
    order(single_variable_models$AUC),
  ]
row.names(single_variable_models) <- NULL
kable(head(single_variable_models))

paste(
  nrow(selected_models),
  "features with an AUC above 0.6 are selected out of",
  nrow(single_variable_models),
  "the features."
)
```

Even though the good single variable models perform quite well in this dataset due to the large amount of levels in them and a reasonably easy binary classification task, both Naive Bayes and Logistic Regression can still take advantage of the high performing variables to make even better predictions. They have quite a similar performance but Logistic Regression has to rely on the probabilities from the single variable models to train because it cannot handle categorical variables with many levels.

```{r, fig.width=5, fig.height=5}
kable(model_evaluations)

roc_plot(naive_bayes_pred, test_set[, target], "red")
roc_plot(logistic_regression_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("red", "blue"),
  c("Naive Bayes", "Logistic Regression"),
  lwd = 2
)
```

# Clustering

The goal of clustering is to discover similarities among subsets of the data. Given a binary classification was previously performed, it will be interesting to see the dataset forms clusters around the salary of job postings. As clustering techniques work better on numerical variables, first extract them and apply scaling. As this dataset contains mostly categorical variables the only numerical variables are salaries and ratings.

```{r}
clustering_df <- scale(
  df[, colnames(df[sapply(df, class) %in% c("numeric", "integer")])]
)
head(clustering_df)
```

## Hierarchical Clustering

Hierarchical Clustering is chosen over kMeans as there is no clear number of clusters expected to be formed from this dataset. The focus is more on exploring possible partitions in the data.

```{r}
hc <- hclust(dist(clustering_df, method = "euclidean"), method = "ward.D2")
```

Plot the dendrogram. It looks like a majority of the data tend to form a big cluster, even though forming 2 or 3 clusters both seem to be quite stable, the big cluster should be divided up as much as possible. Plotting the horizontal lines helps visualise the stability of different number of clusters and 6 seems to be the largest and stable choice.

```{r, fig.width=12, fig.height=8}
hcd <- as.dendrogram(hc)
plot(hcd, ylab = "Height", leaflab = "none", horiz = FALSE)
abline(h = 51.4, col = "red", lty = 2, lwd = 1.3)
abline(h = 43.2, col = "red", lty = 2, lwd = 1.3)
abline(h = 40.5, col = "red", lty = 2, lwd = 1.3)
abline(h = 27.4, col = "red", lty = 2, lwd = 1.3)
text(5, c(47, 41.9, 34), paste("k =", 4:6), col = "blue", cex = 0.7)
```

Cut the dendrogram at different heights to return the cluster sizes. Show the distribution of observations for 1 to 9 clusters.

```{r, message=FALSE}
kmax <- 9
# Number of observations in each class
xtabs(~cluster + max_clust, as.data.frame(cutree(hc, 1:kmax)) %>%
  pivot_longer(
    cols = 1:kmax,
    names_to = "max_clust",
    values_to = "cluster"
  )
)
```

This table indicates the distribution of observations at cluster levels 1 to 9. The dataset has 1905 observations, and this splits into 1751 and 154 at a cluster level of 2. At the third cluster level the dataset is split into 1410, 341 and 154 observations. It is noticeable that the majority of the data shows similiar characteristics, except for the 154 observations which are significantly different as they are always in one cluster by themselves throughout 9 levels. 

## Optimal Number of Clusters

To determine the optimal number of clusters for the dataset, the total *Within Sum of Squares* (WSS) and the *Calinski-Harabasz* index (CH Index) should be measured. An optimal number of clusters is defined such that WSS is minimised and the CH Index is maximised as there is limited variance within clusters and large variance between clusters.

```{r}
# Function to calculate the WSS of a cluster
wss <- function(cluster) {
  c0 <- colMeans(cluster)
  sum(apply(cluster, 1, function(row) sum((row - c0) ^ 2)))
}

# Function to calculate the total WSS
wss_total <- function(df, labels) {
  total <- 0
  for (i in seq(unique(labels))) {
    total <- total + wss(subset(df, labels == i))
  }
  total
}

# Function to calculate the CH indices computed using hierarchical clustering
ch_index <- function(df, kmax, hc) {
  npts <- nrow(df)
  wss_values <- numeric(kmax) # create a vector of numeric type

  # wss_values[1] stores the WSS value for k = 1
  # when all the data points form 1 large cluster
  wss_values[1] <- wss(df)

  for (k in 2:kmax) {
    labels <- cutree(hc, k)
    wss_values[k] <- wss_total(df, labels)
  }

  # CH Index = B / W
  b <- (wss(df) - wss_values) / (0:(kmax - 1))
  w <- wss_values / (npts - seq(kmax))
  data.frame(k = seq(kmax), CH.index = b / w, WSS = wss_values)
}
```

Plot the CH Index and WSS across different k values from 1 to 9 to visualise the optimal number of clusters.

```{r, warning=FALSE, fig.width=10, fig.height=3}
ch_criterion <- ch_index(clustering_df, kmax, hc)
grid.arrange(
  ggplot(ch_criterion, aes(x = k, y = CH.index)) +
    geom_point() +
    geom_line(colour = "red") +
    scale_x_continuous(breaks = 1:kmax, labels = 1:kmax) +
    labs(y = "CH index"),
  ggplot(ch_criterion, aes(x = k, y = WSS), color = "blue") +
    geom_point() + geom_line(colour = "blue") +
    scale_x_continuous(breaks = 1:kmax, labels = 1:kmax),
  ncol = 2
)
```

The CH criterion is maximised at k = 2 and there is an almost local maximum at k = 6, whereas the WSS is minimised at k = 9 but with a slow down in the rate of change from k = 6, which can be considered a reasonable estimate of the optimal number of clusters. Thus 6 clusters seem to be a good choice to maximise the distance between clusters and minimise the variability within clusters.  

As this hypothesis reinforces the choice of 6 clusters being the largest stable one, plot the dendrogram again with the 6 clusters to visualise it.

```{r, fig.width=12, fig.height=8}
plot(hcd, ylab = "Height", leaflab = "none", horiz = FALSE)
rect.hclust(hc, k = 6)
```

## Validating Clusters

Use PCA to project the data into 2D so that the distribution of clusters can be visualised through plotting the convex hulls.

```{r, warning=FALSE, fig.width=10}
pca <- prcomp(clustering_df)
project_2d <- as.data.frame(predict(pca, newdata = clustering_df)[, c(1, 2)])

find_convex_hull <- function(project_2d, clusters) {
  do.call(rbind,
    lapply(
      unique(clusters),
      function(c) {
        f <- subset(project_2d, cluster == c)
        f[chull(f), ]
      }
    )
  )
}

fig <- c()
for (k in 2:7) {
  clusters <- cutree(hc, k)
  project_2d_df <- cbind(
    project_2d,
    cluster = as.factor(clusters),
    salary = df$Estimate.Base.Salary
  )
  convex_hull <- find_convex_hull(project_2d_df, clusters)
  assign(paste0("k", k),
    ggplot(project_2d_df, aes(x = PC1, y = PC2)) +
      geom_point(aes(shape = cluster, color = cluster, alpha = 0.1)) +
      geom_polygon(data = convex_hull, aes(group = cluster, fill = cluster),
      alpha = 0.4, linetype = 0) +
      labs(title = sprintf("k = %d", k)) +
      theme(legend.position = "none")
  )
}

grid.arrange(k2, k3, k4, k5, k6, k7, ncol = 3)
```

As evident in the figure above, the data is first split into 2 clusters, the one on the left demonstrates significant variability as increasing the number of clusters always divides it into smaller clusters. On the other hand the right cluster never changes since the first split. 

```{r, results=FALSE}
# Find out how stable the clusters are
clusterboot_hclust <- clusterboot(
  clustering_df,
  clustermethod = hclustCBI,
  method = "ward.D2",
  k = 6
)
```

Except for 1 cluster with a stability of 0.46, every other clusters are highly stable as their values are close to 1. This matches the cluster distribution above for k = 2 to 7 where that 1 big cluster is formed at the beginning and as the number of clusters increases, it is dissolved into smaller clusters while the others stay the same once the cluster is formed.

```{r}
kable(data.frame(
  Cluster = seq(clusterboot_hclust$bootbrd),
  Stability = 1 - clusterboot_hclust$bootbrd / 100
))
```

## Exploring Clusters 

Now that the optimal number of clusters is found, use the 6 clusters to explore the patterns they represent in the data.

```{r}
# Append the cluster number to the original dataset
df$Cluster <- as.factor(cutree(hc, 6))
```

### Job Locations

Plot a filled bar chart to investigate whether each cluster tend to be in the same state.

```{r}
ggplot(df) +
  geom_bar(
    aes(x = Cluster, fill = State),
    alpha = 0.7,
    position = "fill"
  )
```

Unfortunately there is no geographical pattern to be found in the clusters as all of the states have some percentage of data in each cluster and they are quite evenly spread out.

### Salary

Plot a histogram to investigate whether the clusters form around around different levels of salary. 

```{r}
ggplot(df) +
  geom_histogram(
    aes(x = Estimate.Base.Salary, fill = Cluster),
    binwidth = 8000,
    alpha = 0.7
  ) +
  scale_x_continuous(
    labels = scales::dollar_format(),
    breaks = seq(0, 300000, 20000)
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.title.x = element_text(margin = margin(t = 10))
  )
```

Cluster 1 is the largest cluster and covers salaries between $60,000 to $108,000. Cluster 6 predominantly covers salaries in the range of $110,000 to $170,000. Cluster 3 covers the high end salaries over $180,000. Rest of the 3 clusters are overlapping in the lower end of the salaries below $60,000, but for the most part the figure demonstrates a separation of the data into "low", "medium", "high", and "very high" salaries.

### Rating

Plot a scatter plot to investigate whether the company ratings have any relationships with the clusters formed.

```{r}
ggplot(df) +
  geom_point(
    aes(
      x = Company.Rating,
      y = Estimate.Base.Salary,
      colour = Cluster
    ),
    alpha = 0.7
  ) +
  scale_y_continuous(labels = scales::dollar_format()) +
  theme(axis.title.y = element_text(margin = margin(r = 5)))
```

There is a much clear separation between clusters here. Cluster 5 contains all the low ratings of 1 as well as most of the ratings below 2.5. Cluster 2 is situated around higher ratings above 4.5. Cluster 4 centers at the medium rating of 3. Cluster 3 as the smallest cluster mostly contains the very high salaries with ratings between 4 and 5. Cluster 1 being the largest cluster covers the ratings around 4 but the lower end of the salaries while cluster 6 has the higher end of the same ratings.

This separation comes from the fact that the only numerical variables in this dataset are the salaries and ratings so they are the determining factors in the clustering process.
