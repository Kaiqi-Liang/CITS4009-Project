---
title: "CITS4009 Computational Data Analysis"
subtitle: "Project 2 - Modelling"

graphics: yes
author: <i>Kaiqi LIANG (23344153) - Briana DAVIES-MORRELL (22734723)</i>
date: "Semester 2, 2022"

output:
  html_document:
    includes:
      before_body: style.html
    number_sections: true
---

# Introduction

The dataset for this project is Australia's data science job listings for August 2022 which can be downloaded on [Kaggle](https://www.kaggle.com/datasets/nadzmiagthomas/australia-data-science-jobs). It was scrapped from [Glassdoor](https://www.glassdoor.com.au) which is a website where current and former employees anonymously review companies.

# Setup

Clean workspace and set random seed.

```{r}
rm(list = ls())
set.seed(1)
```

Code to allow truncating text output especially for `str()` and `head()` functions.

```{r}
# Save the built-in output hook
hook_output <- knitr::knit_hooks$get("output")

# Set a new output hook to truncate text output
knitr::knit_hooks$set(output = function(x, options) {
  if (!is.null(n <- options$out.lines)) {
    x <- xfun::split_lines(x)
    if (length(x) > n) {
      # Truncate the output
      x <- c(head(x, n), "....\n")
    }
    x <- paste(x, collapse = "\n")
  }
  hook_output(x, options)
})
```

Load libraries and data.

```{r, message=FALSE}
library(dplyr)
library(ROCR)
library(ggplot2)
library(gridExtra)
library(ROCit)
library(caret)
library(lime)
library(knitr)
library(grDevices)
library(tidyverse)
library(fpc)
library(klaR)

df <- read.csv(file = "AustraliaDataScienceJobs.csv", header = TRUE)
```

# Data Preparation

Firstly, clean the dataset and remove or impute missing values.

```{r, out.lines=25}
# Replace empty strings with NA
df[df == ""] <- NA

# Fix typos and shortern variable names
df <- rename(df,
  Number.of.Rater = Companny.Number.of.Rater,
  Career.Opportunities = Company.Career.Opportinities,
  Culture.and.Values = Company.Culture.and.Values,
  Senior.Management = Company.Senior.Management,
  Work.Life.Balance = Company.Work.Life.Balance,
  Friend.Recommendation = Company.Friend.Reccomendation
)

# Drop columns
df <- df[!names(df) %in% c(
    "Country", # always Australia
    "Job.Descriptions", # text based
    "Url", # similar to an ID
    "Company.Founded", # dependent on Company which does not provide new info
    "Company.CEO.Approval", # too many missing values
    "Number.of.Rater" # not valuable
)]

# Convert _yn columns to logical
skill_columns <- grep("_yn", names(df))
df[skill_columns] <- sapply(df[skill_columns], function(col) {
  as.logical(col)
})

# Drop rows that contain NAs in either Job.Title or Company.Size
df <- df[!is.na(df$Job.Title) & !is.na(df$Company.Size), ]

# Impute NAs with the median value in the rating columns
rating_columns <- c(
  "Company.Rating",
  "Career.Opportunities",
  "Compensation.and.Benefits",
  "Culture.and.Values",
  "Senior.Management",
  "Work.Life.Balance",
  "Friend.Recommendation"
)
df[rating_columns] <- sapply(df[rating_columns], function(column) {
  na <- is.na(column)
  column[na] <- median(!na)
  column
})

# Convert NAs to Unknown in Company Sector and Company Industry
industry_column <- c("Company.Sector", "Company.Industry")
df[industry_column] <- sapply(df[industry_column], function(column) {
  column[is.na(column)] <- "Unknown"
  column
})

# Change the location Australia to Unknown as all the other locations are
# cities other than countries.
df$Job.Location[df$Job.Location == "Australia"] <- "Unknown"

str(df)
```

# Classification

## Target Variable

Sometimes job postings do not include the salary. A classification model that predicts the salary range of future job postings, based on past job postings with salary information, would be incredibly useful for job hunters who prioritise salary when applying for jobs. As such, the response variable we have chosen is the salary level of the job posting.

The original dataset included three numeric salary variables: `High.Estimate`, `Estimate.Base.Salary`, and `Low.Estimate`. In job descriptions the salary package is often provided as a range rather than a fixed number as there is room for negotiation, the lower end of the range is `Low.Estimate` and the higher end is `High.Estimate`, `Estimate.Base.Salary` is the average of the two. Hence focusing on the `Estimate.Base.Salary` this can be a binary classification problem of whether the salary is high. The cutoff between these two classes is the median of the `Estimate.Base.Salary`, $95,000, so one class has a salary between $0 - $95,000 and is assigned a label of `FALSE`, the other is a salary above $95,000 and is assigned a label of `TRUE`. Since the cutoff is based on the median, the dataset is almost balanced.

```{r}
target <- "High.Salary"
df[, target] <- df$Estimate.Base.Salary >= median(df$Estimate.Base.Salary)

paste(
  "There are",
  nrow(df[df[, target], ]),
  "high salary observations. There are",
  nrow(df[!df[, target], ]),
  "low salary observations."
)
```

## Feature Variables

Identify the categorical and numerical feature variables.

```{r}
features <- setdiff(colnames(df), target)

# Convert character to factor
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)

categorical_variables <- features[
  sapply(df[, features], class) %in% c("factor", "logical")
]
numerical_variables <- features[
  sapply(df[, features], class) %in% "numeric"
]

paste(
  "There are",
  length(categorical_variables),
  "categorical features,",
  length(numerical_variables),
  "numerical features",
  "and 1 target column."
)
```

Convert numerical variables to categorical so that they can be used to create single variable models, but still keep the original numerical variables for clustering.

```{r}
numerical_to_categorical <- c()
for (variable in numerical_variables) {
  categorical_variable <- paste(variable, "categorical", sep = "_")
  numerical_to_categorical <- c(numerical_to_categorical, categorical_variable)
  if (variable == "Friend.Recommendation") { # A rating out of 100
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 100, 10)
    )
  } else { # Ratings out of 5
    df[categorical_variable] <- cut(
      df[, variable],
      seq(0, 5, 0.5)
    )
  }
}
```

## Test and Training Sets

Perform a 80/20 random split on the dataset to get a training and test set.

```{r}
split <- runif(nrow(df))
training_set <- df[split < 0.8, ]
test_set <- df[split >= 0.8, ]

paste(
  "The training and test set has",
  nrow(training_set),
  "and",
  nrow(test_set),
  "observations respectively."
)
```

## Null model

The null model will always return the the majority category. As mentioned earlier, the dataset is almost balanced so the model will have a 50% chance of predicting `High.Salary (TRUE)`, resulting in an Area Under the Curve (AUC) of 0.5.

```{r}
calc_auc <- function(pred, ground_truth) {
  round(as.numeric(
    performance(prediction(pred, ground_truth), "auc")@y.values
  ), 4)
}

calc_log_likelihood <- function(pred, ground_truth) {
  pred <- pred[pred > 0 & pred < 1]
  round(sum(ifelse(ground_truth, log(pred), log(1 - pred))))
}

null_model <- sum(training_set[target]) / nrow(training_set)
```

Create a data frame to store the AUC and Log Likelihood of different models.

```{r}
# Function to calculate the AUC and Log Likelihood then store their values
# to the global data frame which contains the same values for other models
calc_auc_log_likelihood <- function(pred, name, type) {
  auc <- calc_auc(pred, test_set[target])
  log_likelihood <- calc_log_likelihood(pred, test_set[, target])
  print(paste("AUC:", auc))
  print(paste("Log Likelihood:", log_likelihood))

  model_evaluations[nrow(model_evaluations) + 1, ] <- c(
    name,
    type,
    auc,
    log_likelihood
  )
  assign("model_evaluations", model_evaluations, envir = .GlobalEnv)
}

model_evaluations <- data.frame(
  Model.Name = "Null Model",
  Model.Type = "univariate",
  AUC = calc_auc(rep(null_model, nrow(test_set)), test_set[target]),
  Log.Likelihood = calc_log_likelihood(null_model, test_set[, target])
)
kable(model_evaluations)
```

## Single Variable Model

Single variable model based on categorical variables.

```{r}
single_variable_prediction <- function(pred_col, output_col, test_col) {
  t <- table(pred_col, output_col)
  pred <- (t[, 2] / (t[, 1] + t[, 2]))[as.character(test_col)]
  pred[is.na(pred)] <- sum(output_col) / length(output_col)
  pred
}

cross_validation_100_fold <- function(variable) {
  aucs <- rep(0, 100)
  for (i in seq(aucs)) {
    split <- rbinom(n = nrow(training_set), size = 1, prob = 0.1) == 1
    pred_col <- single_variable_prediction(
      training_set[split, variable],
      training_set[split, target],
      training_set[!split, variable]
    )
    aucs[i] <- calc_auc(pred_col, training_set[!split, target])
  }
  mean(aucs)
}
```

Find the average AUC for each variable over 100 fold cross validation and save the predicted probabilities back to the data frame so that they can be used in Logistic Regression.

```{r}
single_variable_models <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(single_variable_models) <- c("Variable", "AUC")
for (variable in c(numerical_to_categorical, categorical_variables)) {
  auc <- cross_validation_100_fold(variable)
  single_variable_models[nrow(single_variable_models) + 1, ] <-
    c(variable, auc)

  training_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      training_set[, variable]
    )

  test_set[paste(variable, "pred", sep = "_")] <-
    single_variable_prediction(
      training_set[, variable],
      training_set[, target],
      test_set[, variable]
    )
}
```

Select the variables with an average AUC higher than 0.6 to be the features used in model trainings later on.

```{r}
selected_models <-
  single_variable_models[
    single_variable_models$AUC > 0.6,
  ]
selected_features <- selected_models$Variable

selected_models <-
  selected_models[
    order(selected_models$AUC, decreasing = TRUE),
  ]
row.names(selected_models) <- NULL

kable(selected_models)
```

Pick the top 2 single variable models based on their average AUC which are `Company` and `Job.Title` having a unusually high value of almost 0.9 and 0.8. However this makes perfect sense as within the same company most data science jobs will very likely to have the same salary, and similarly the exact same job title usually would not have a big difference in their salaries.

```{r}
company_pred <- single_variable_prediction(
  training_set$Company,
  training_set[, target],
  test_set$Company
)
job_title_pred <- single_variable_prediction(
  training_set$Job.Title,
  training_set[, target],
  test_set$Job.Title
)

# Calculate their AUC and Log Likelihood using the test set.
calc_auc_log_likelihood(company_pred, "Company", "univariate")
calc_auc_log_likelihood(job_title_pred, "Job Title", "univariate")
```

The metrics measured on the test set are higher than on the validation set because the latter was the average across 100 folds.

Plot their predicted probabilities next to each other.

```{r, fig.width=10, fig.height=3}
double_density_plot <- function(
  pred_col,
  output_col,
  x,
  y
) {
  ggplot(data.frame(
    pred = pred_col,
    High.Salary = output_col
  )) +
    geom_density(aes(x = pred, colour = High.Salary)) +
    labs(x = paste("Predicated Probability of", x), y = y)
}

grid.arrange(
  double_density_plot(
    company_pred,
    test_set[target],
    "Company",
    "Density"
  ),
  double_density_plot(
    job_title_pred,
    test_set[target],
    "Job Title",
    ""
  ),
  ncol = 2
)
```

Compare the ROC curves by plotting them on top of each other.

```{r, fig.width=5, fig.height=5}
roc_plot <- function(pred_col, out_col, colour = "red", overlaid = FALSE) {
  par(new = overlaid)
  plot(
    rocit(score = pred_col, class = out_col),
    col = c(colour, "black"),
    legend = FALSE,
    YIndex = FALSE
  )
}

roc_plot(company_pred, test_set[, target], "red")
roc_plot(job_title_pred, test_set[, target], "blue", TRUE)
legend(
  "bottomright",
  col = c("red", "blue"),
  c("Company", "Job Title"),
  lwd = 2
)
```

As shown above using the AUC metric, `Company` performs moderately better than `Job.Title` however using the Log Likelihood metric, `Job.Title` is better. This means `Company` has a higher performance averaged across all possible decision thresholds but `Job.Title` gives a higher certainty in its predictions as Log Likelihood measures how close the predicted probabilities are to the ground truth (0 or 1).

## Model Evaluation

Functions to call on a model to make predictions, calculate the AUC and Log Likelihood, evaluate which features play a key role in determining if a job posting offers high or low salary.

```{r}
# Function to print the confusion matrix as well as calculating
# the precision and recall
confusion_matrix_accuracy <- function(model, features) {
  pred <- as.logical(predict(
    model,
    test_set[features],
  ))

  confusion_matrix <- table(
    ifelse(test_set[, target], "High Salary", "Low Salary"),
    pred
  )[, 2:1]

  print(paste(
    "Precision:",
    format(confusion_matrix[1, 1] / sum(confusion_matrix[, 1]), digits = 3)
  ))

  print(paste(
    "Recall:",
    format(confusion_matrix[1, 1] / sum(confusion_matrix[1, ]), digits = 3)
  ))

  print(kable(confusion_matrix))
  pred
}

# Function to calculate the AUC and Log Likelihood as well as generating a
# double density plot
evaluate_model <- function(model, features, name) {
  pred <- predict(
    model,
    test_set[features],
    "prob"
  )[2]
  pred <- unlist(pred)

  calc_auc_log_likelihood(pred, name, "multivariate")

  plot(double_density_plot(
    pred,
    test_set[target],
    name,
    "Density"
  ))
  pred
}

# Function to plot the explanation of how the individual features
# support or contradict a prediction
lime_plot <- function(model, features, pred) {
  # Pick 4 examples for LIME to explain
  test_cases <- c()

  # True Positive
  for (i in seq(length(pred))) {
    if (test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Negative
  for (i in seq(length(pred))) {
    if (test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # False Positive
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  # True Negative
  for (i in seq(length(pred))) {
    if (!test_set[i, target] && !pred[i]) {
      test_cases <- c(test_cases, i)
      break
    }
  }

  example <- test_set[test_cases, features]
  explainer <- lime(
    training_set[features],
    model = model,
    bin_continuous = TRUE,
    n_bins = 10
  )

  explanation <- explain(
    example,
    explainer,
    n_labels = 1,
    n_features = length(features)
  )
  plot_features(explanation)
}
```

The LIME plot will explain 4 test cases, top left is a True Positive instance, top right is a False Negative instance, bottom left is a False Positive instance and bottom right is a True Negative instance.

## Naive Bayes

Naive Bayes classifier works well on categorical variables which is why it is chosen for this problem as there are many categorical variables with lots of levels.

```{r, warning=FALSE}
naive_bayes <- caret::train(
  x = training_set[selected_features],
  y = as.factor(training_set[, target]),
  method = "nb"
)
```

Naive Bayes has an AUC of 0.9667 and a Log Likelihood of -116 which outperforms the highest single variable model of `Company` with an AUC of 0.904 and a Log Likelihood of -401. This means Naive Bayes is doing a great job of combining the top performing variables to improve its predictions even further.

```{r, warning=FALSE, fig.width=5, fig.height=3}
naive_bayes_pred <- evaluate_model(
  naive_bayes,
  selected_features,
  "Naive Bayes"
)
```

The number of False Positive and False Negative cases are the same which results in the exact same precision and recall.

```{r, warning=FALSE}
pred <- confusion_matrix_accuracy(naive_bayes, selected_features)
```

Except for the False Negative case shown by the top right LIME plot, 2 of the No.1 determining features are `Company` which is the top performing single variable model, and the other is `Job.Title` which is the second best variable.

```{r, warning=FALSE, fig.width=12, fig.height=8}
lime_plot(naive_bayes, selected_features, pred)
```

## Logistic Regression

Logistic Regression can be used to classify a variable dependent on one or more independent features. It will find the best fitting model to describe the relationship between the dependent and the independent variables. As it is a binary classification task `binomial` distribution is used.

Due to every categorical variable has to be expanded to a set of indicator variables in Logistic Regression, it does not work well with large number of levels. Therefore instead of using the original data whose categorical variables contain many levels, the predicted probabilities from the single variable models will be used.

```{r, warning=FALSE}
probability_columns <- paste(selected_features, "pred", sep = "_")
logistic_regression <- caret::train(
  x = training_set[probability_columns],
  y = as.factor(training_set[, target]),
  method = "glm",
  family = binomial(link = "logit")
)
```

Logistic Regression has an AUC of 0.9672 and a Log Likelihood of -120 which is pretty much the same as Naive Bayes. However the difference between them is in the precision and recall.

```{r, warning=FALSE, fig.width=5, fig.height=3}
logistic_regression_pred <- evaluate_model(
  logistic_regression,
  probability_columns,
  "Logistic Regression"
)
```

Logistic Regression has a lower precision but a higher recall than Naive Bayes which means it is able to identify more of the high salary jobs than Naive Bayes while making more mistakes predicting low salary jobs as high salary.

```{r}
pred <- confusion_matrix_accuracy(logistic_regression, probability_columns)
```

Once again except for the False Negative case shown by the top right LIME plot, the predicted probabilities from the `Company` single varible model is the most supporting feature which is also the top performing variable.

```{r, warning=FALSE, fig.width=12, fig.height=8}
lime_plot(logistic_regression, probability_columns, pred)
```

The top 2 highest performing single model variables `Company` and `Job.Title` are also indicated as the 2 most significant variables shown under the *Coefficients* part of the summary as their p-value are much smaller than others.

```{r}
summary(logistic_regression)
```

## Comparison

There are a couple single variable models that perform even worse than the null model which has an AUC of 0.5 on this balanced dataset. However the majority of them perform quite well and only the AUCs above 0.6 will be used as the features for training the classification models. Therefore these low performing features will not affect the performance of the models.

```{r}
paste0(
  round(
    nrow(single_variable_models[single_variable_models$AUC > 0.5, ]) /
    nrow(single_variable_models) *
    100
  ),
  "% of the single variable models perform better than the null model."
)

single_variable_models <-
  single_variable_models[
    order(single_variable_models$AUC),
  ]
row.names(single_variable_models) <- NULL
kable(head(single_variable_models))

paste(
  nrow(selected_models),
  "features with an AUC above 0.6 are selected out of",
  nrow(single_variable_models),
  "the features."
)
```

Even though the good single variable models perform quite well in this dataset due to the large amount of levels in them and a reasonably easy binary classification task, both Naive Bayes and Logistic Regression can still take advantage of the high performing variables to make even better predictions. They have quite a similar performance but Logistic Regression has to rely on the probabilities from the single variable models to train because it cannot handle categorical variables with many levels.

```{r, fig.width=5, fig.height=5}
kable(model_evaluations)

roc_plot(naive_bayes_pred, test_set[, target], "red")
roc_plot(logistic_regression_pred, test_set[, target], "blue", TRUE)

legend(
  "bottomright",
  col = c("red", "blue"),
  c("Naive Bayes", "Logistic Regression"),
  lwd = 2
)
```

# Clustering

The goal of clustering is to discover similarities among subsets of the data. Given a binary classification was previously performed, it will be interesting to see the dataset forms clusters around the salary of job postings. As clustering techniques work better on numerical variables, first extract them and apply scaling. As this dataset contains mostly categorical variables the only numerical variables are salaries and ratings.

```{r}
clustering_df <- scale(
  df[, colnames(df[sapply(df, class) %in% c("numeric", "integer")])]
)
head(clustering_df)
```

## Hierarchical Clustering

Hierarchical Clustering is chosen over kMeans as there is no clear number of clusters expected to be formed from this dataset. The focus is more on exploring possible partitions in the data.

```{r}
hc <- hclust(dist(clustering_df, method = "euclidean"), method = "ward.D2")
```

Plot the dendrogram. It looks like a majority of the data tend to form a big cluster, even though forming 2 or 3 clusters both seem to be quite stable, the big cluster should be divided up as much as possible. Plotting the horizontal lines helps visualise the stability of different number of clusters and 6 seems to be the largest and stable choice.

```{r, fig.width=12, fig.height=8}
hcd <- as.dendrogram(hc)
plot(hcd, ylab = "Height", leaflab = "none", horiz = FALSE)
abline(h = 51.4, col = "red", lty = 2, lwd = 1.3)
abline(h = 43.2, col = "red", lty = 2, lwd = 1.3)
abline(h = 40.5, col = "red", lty = 2, lwd = 1.3)
abline(h = 27.4, col = "red", lty = 2, lwd = 1.3)
text(5, c(47, 41.9, 34), paste("k =", 4:6), col = "blue", cex = 0.7)
```

Cut the dendrogram at different heights to return the cluster sizes. Show the distribution of observations for 1 to 9 clusters.

```{r, message=FALSE}
kmax <- 9
# Number of observations in each class
xtabs(~cluster + max_clust, as.data.frame(cutree(hc, 1:kmax)) %>%
  pivot_longer(
    cols = 1:kmax,
    names_to = "max_clust",
    values_to = "cluster"
  )
)
```

This table indicates the distribution of observations at cluster levels 1 to 9. The dataset has 1905 observations, and this splits into 1751 and 154 at a cluster level of 2. At the third cluster level the dataset is split into 1410, 341 and 154 observations. It is noticeable that the majority of the data shows similiar characteristics, except for the 154 observations which are significantly different as they are always in one cluster by themselves throughout 9 levels. 

## Optimal Number of Clusters

To determine the optimal number of clusters for the dataset, the total *Within Sum of Squares* (WSS) and the *Calinski-Harabasz* index (CH Index) should be measured. An optimal number of clusters is defined such that WSS is minimised and the CH Index is maximised as there is limited variance within clusters and large variance between clusters.

```{r}
# Function to calculate the WSS of a cluster
wss <- function(cluster) {
  c0 <- colMeans(cluster)
  sum(apply(cluster, 1, function(row) sum((row - c0) ^ 2)))
}

# Function to calculate the total WSS
wss_total <- function(df, labels) {
  total <- 0
  for (i in seq(unique(labels))) {
    total <- total + wss(subset(df, labels == i))
  }
  total
}

# Function to calculate the CH indices computed using hierarchical clustering
ch_index <- function(df, kmax, hc) {
  npts <- nrow(df)
  wss_values <- numeric(kmax) # create a vector of numeric type

  # wss_values[1] stores the WSS value for k = 1
  # when all the data points form 1 large cluster
  wss_values[1] <- wss(df)

  for (k in 2:kmax) {
    labels <- cutree(hc, k)
    wss_values[k] <- wss_total(df, labels)
  }

  # CH Index = B / W
  b <- (wss(df) - wss_values) / (0:(kmax - 1))
  w <- wss_values / (npts - seq(kmax))
  data.frame(k = seq(kmax), CH.index = b / w, WSS = wss_values)
}
```

Plot the CH Index and WSS across different k values from 1 to 9 to visualise the optimal number of clusters.

```{r, warning=FALSE, fig.width=10, fig.height=3}
ch_criterion <- ch_index(clustering_df, kmax, hc)
grid.arrange(
  ggplot(ch_criterion, aes(x = k, y = CH.index)) +
    geom_point() +
    geom_line(colour = "red") +
    scale_x_continuous(breaks = 1:kmax, labels = 1:kmax) +
    labs(y = "CH index"),
  ggplot(ch_criterion, aes(x = k, y = WSS), color = "blue") +
    geom_point() + geom_line(colour = "blue") +
    scale_x_continuous(breaks = 1:kmax, labels = 1:kmax),
  ncol = 2
)
```

The CH criterion is maximised at k = 2 and there is an almost local maximum at k = 6, whereas the WSS is minimised at k = 9 but with a slow down in the rate of change from k = 6, which can be considered a reasonable estimate of the optimal number of clusters. Thus 6 clusters seem to be a good choice to maximise the distance between clusters and minimise the variability within clusters.  

As this hypothesis reinforces the choice of 6 clusters being the largest stable one, plot the dendrogram again with the 6 clusters to visualise it.

```{r, fig.width=12, fig.height=8}
plot(hcd, ylab = "Height", leaflab = "none", horiz = FALSE)
rect.hclust(hc, k = 6)
```

## Validating Clusters

Use PCA to project the data into 2D so that the distribution of clusters can be visualised through plotting the convex hulls.

```{r, warning=FALSE, fig.width=10}
pca <- prcomp(clustering_df)
project_2d <- as.data.frame(predict(pca, newdata = clustering_df)[, c(1, 2)])

find_convex_hull <- function(project_2d, clusters) {
  do.call(rbind,
    lapply(
      unique(clusters),
      function(c) {
        f <- subset(project_2d, cluster == c)
        f[chull(f), ]
      }
    )
  )
}

fig <- c()
for (k in 2:7) {
  clusters <- cutree(hc, k)
  project_2d_df <- cbind(
    project_2d,
    cluster = as.factor(clusters),
    salary = df$Estimate.Base.Salary
  )
  convex_hull <- find_convex_hull(project_2d_df, clusters)
  assign(paste0("k", k),
    ggplot(project_2d_df, aes(x = PC1, y = PC2)) +
      geom_point(aes(shape = cluster, color = cluster, alpha = 0.1)) +
      geom_polygon(data = convex_hull, aes(group = cluster, fill = cluster),
      alpha = 0.4, linetype = 0) +
      labs(title = sprintf("k = %d", k)) +
      theme(legend.position = "none")
  )
}

grid.arrange(k2, k3, k4, k5, k6, k7, ncol = 3)
```

As evident in the figure above, the data is first split into 2 clusters, the one on the left demonstrates significant variability as increasing the number of clusters always divides it into smaller clusters. On the other hand the right cluster never changes since the first split. 

```{r, results=FALSE}
# Find out how stable the clusters are
clusterboot_hclust <- clusterboot(
  clustering_df,
  clustermethod = hclustCBI,
  method = "ward.D2",
  k = 6
)
```

Except for 1 cluster with a stability of 0.46, every other clusters are highly stable as their values are close to 1. This matches the cluster distribution above for k = 2 to 7 where that 1 big cluster is formed at the beginning and as the number of clusters increases, it is dissolved into smaller clusters while the others stay the same once the cluster is formed.

```{r}
kable(data.frame(
  Cluster = seq(clusterboot_hclust$bootbrd),
  Stability = 1 - clusterboot_hclust$bootbrd / 100
))
```

## Exploring Clusters 

Now that the optimal number of clusters is found, use the 6 clusters to explore the patterns they represent in the data.

```{r}
# Append the cluster number to the original dataset
df$Cluster <- as.factor(cutree(hc, 6))
```

### Job Locations

Plot a filled bar chart to investigate whether each cluster tend to be in the same state.

```{r}
ggplot(df) +
  geom_bar(
    aes(x = Cluster, fill = State),
    alpha = 0.7,
    position = "fill"
  )
```

Unfortunately there is no geographical pattern to be found in the clusters as all of the states have some percentage of data in each cluster and they are quite evenly spread out.

### Salary

Plot a histogram to investigate whether the clusters form around around different levels of salary. 

```{r}
ggplot(df) +
  geom_histogram(
    aes(x = Estimate.Base.Salary, fill = Cluster),
    binwidth = 8000,
    alpha = 0.7
  ) +
  scale_x_continuous(
    labels = scales::dollar_format(),
    breaks = seq(0, 300000, 20000)
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.title.x = element_text(margin = margin(t = 10))
  )
```

Cluster 1 is the largest cluster and covers salaries between $60,000 to $108,000. Cluster 6 predominantly covers salaries in the range of $110,000 to $170,000. Cluster 3 covers the high end salaries over $180,000. Rest of the 3 clusters are overlapping in the lower end of the salaries below $60,000, but for the most part the figure demonstrates a separation of the data into "low", "medium", "high", and "very high" salaries.

### Rating

Plot a scatter plot to investigate whether the company ratings have any relationships with the clusters formed.

```{r}
ggplot(df) +
  geom_point(
    aes(
      x = Company.Rating,
      y = Estimate.Base.Salary,
      colour = Cluster
    ),
    alpha = 0.7
  ) +
  scale_y_continuous(labels = scales::dollar_format()) +
  theme(axis.title.y = element_text(margin = margin(r = 5)))
```

There is a much clear separation between clusters here. Cluster 5 contains all the low ratings of 1 as well as most of the ratings below 2.5. Cluster 2 is situated around higher ratings above 4.5. Cluster 4 centers at the medium rating of 3. Cluster 3 as the smallest cluster mostly contains the very high salaries with ratings between 4 and 5. Cluster 1 being the largest cluster covers the ratings around 4 but the lower end of the salaries while cluster 6 has the higher end of the same ratings.

This separation comes from the fact that the only numerical variables in this dataset are the salaries and ratings so they are the determining factors in the clustering process.
